{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"negative_sampling_layer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMohmpjIdBQIDpyG+h98CXX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"d0aSg2idcssE"},"source":["import sys\n","sys.path.append('..')\n","from common.np import *  # import numpy as np\n","from common.layers import Embedding, SigmoidWithLoss\n","import collections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2i2qYW7ng4EJ"},"source":["class EmbeddingDot:\n","    def __init__(self,W):\n","        self.embed=Embedding(W)\n","        self.params=self.embed.params\n","        self.grads=self.embed.grads\n","\n","    def forward(self,h,idx):\n","        target_W=self.embed.forward(idx)\n","        # print('target_W\\n',target_W)\n","        out=np.sum(target_W*h,axis=1)\n","        self.cache=(h,target_W)\n","        return out\n","\n","    def backward(self,dout):\n","        h,target_W=self.cache\n","        dout=dout.reshape(dout.shape[0],1)  #dtarget_Wh=np.ones(dout.shape[0],1)*dout\n","        dtarget_W=dout*h\n","        self.embed.backward(dtarget_W)\n","        dh=dout*target_W\n","        return dh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_c-Dq1Os3pK3"},"source":["class UnigramSampler:\n","    '''\n","    vocab_size(語彙数)のカウント、negative_sample(負例)の選択確率の導出、負例の選択を行う\n","    '''\n","    def __init__(self, corpus, power, sample_size):\n","        self.sample_size = sample_size\n","        self.vocab_size = None\n","        self.word_p = None\n","\n","        #vocab_size(語彙数)をカウントする\n","        counts = collections.Counter()  #配列の各要素の個数をカウントして辞書化するクラス{you:3,say:4,...}\n","        for word_id in corpus:\n","            counts[word_id] += 1\n","\n","        vocab_size = len(counts)\n","        self.vocab_size = vocab_size\n","\n","        #各語彙の確率分布(word_p)を求める\n","        self.word_p = np.zeros(vocab_size)  #語彙数分の行列を用意する\n","        for i in range(vocab_size):\n","            self.word_p[i] = counts[i]  #各語彙のカウントした数を代入する\n","\n","        self.word_p = np.power(self.word_p, power)  #各数のpower乗をとる.低い確率を少し底上げする（例：power=0.75）\n","        self.word_p /= np.sum(self.word_p)  #power乗した個数の総和で割り、確率化する\n","\n","    def get_negative_sample(self, target):\n","        batch_size = target.shape[0]\n","\n","        if not GPU:\n","            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n","\n","            for i in range(batch_size):\n","                p = self.word_p.copy()  #確率分布配列を深いコピー\n","                target_idx = target[i]  #正例のミニバッチのi番目の要素（word_id）をtarget_idxに代入\n","                p[target_idx] = 0   #正例の確率を０にする\n","                p /= p.sum()    #各確率を負例の選択確率として再計算\n","                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)    #確率分布に従って負例を選択する\n","        else:\n","            # GPU(cupy）で計算するときは、速度を優先\n","            # 負例にターゲットが含まれるケースがある\n","            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n","                                               replace=True, p=self.word_p)\n","\n","        return negative_sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tp3XHIFba550"},"source":["class NegativeSamplingLoss:\n","    def __init__(self,W,corpus,power=0.75,sample_size=5):\n","        self.sample_size=sample_size\n","        self.sampler=UnigramSampler(corpus,power,sample_size)\n","        self.loss_layers=[SigmoidWithLoss() for _ in range(sample_size+1)]   #負例サンプルサイズ＋正例用のレイヤひとつを生成\n","        self.embed_dot_layers=[EmbeddingDot(W) for _ in range(sample_size+1)]   #負例サンプルサイズ＋正例用のレイヤひとつを生成\n","\n","        self.params,self.grads=[],[]\n","        for layer in self.embed_dot_layers:\n","            self.params+=layer.params\n","            self.grads+=layer.grads\n","\n","    def forward(self,h,target):\n","        batch_size=target.shape[0]\n","        negative_sample=self.sampler.get_negative_sample(target)\n","\n","        #正例のフォワード\n","        score=self.embed_dot_layers[0].forward(h,target)    #最初のレイヤを正例用のレイヤとして使用する\n","        correct_label=np.ones(batch_size,dtype=np.int32)    #正解ラベル1をバッチ数だけ用意する\n","        loss=self.loss_layers[0].forward(score,correct_label)\n","        \n","        #負例のフォワード\n","        negative_label=np.zeros(batch_size,dtype=np.int32)  #不正解ラベル０をバッチ数だけ用意する\n","        for i in range(self.sample_size):\n","            negative_target=negative_sample[:,i]\n","            score=self.embed_dot_layers[1+i].forward(h,negative_target)\n","            loss+=self.loss_layers[1+i].forward(score,negative_label)\n","\n","        return loss\n","\n","    def backward(self,dout=1):\n","        dh=0\n","        for l0,l1 in zip(self.loss_layers,self.embed_dot_layers):\n","            dscore=l0.backward(dout)\n","            dh+=l1.backward(dscore)\n","\n","        return dh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8pyLPuEd2Bp"},"source":[""],"execution_count":null,"outputs":[]}]}