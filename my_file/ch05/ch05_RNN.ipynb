{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch05_RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgH2UqcNuPUkOv4JvocPOZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aXnML5zP4yuN"},"source":["##まとめ\n","* RNNはループする経路があり、それによって隠れ状態を内部に記憶することができる\n","* RNNのループ経路を展開することで、複数のRNNレイヤがつながったネットワークと解釈することができ、通常の誤差逆伝搬法によって学習することができる(=BPTT)\n","* 長い時系列データを学習する場合は、適当長さでデータのまとまりを作り（これをブロックという）、ブロック単位でBPTTによる学習を行う（=Truncated BPTT）\n","* Truncated BPTTでは逆伝搬のつながりのみ切断する\n","* Truncated BPTTでは順伝搬のつながりは維持するため、データはシーケンシャルに与える必要がある\n","* 言語モデルは単語の並びを確率として解釈する\n","* RNNレイヤを利用した条件付き言語モデルは、（理論的には）それまで登場した単語の情報を記憶することができる"]},{"cell_type":"code","metadata":{"id":"Z5Fskg2yujfW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608266453959,"user_tz":-540,"elapsed":24048,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"acd8d2a5-8250-464c-d8e6-a38194f845ea"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","ROOT_PATH='/content/drive/My Drive/Colab Notebooks/zero_DL_2'\n","sys.path.append(ROOT_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z7Zzg1Yv1VgZ"},"source":["##確率と言語モデル\n"]},{"cell_type":"markdown","metadata":{"id":"1Ec9QzKF1pId"},"source":["###word2vecを確率の視点から眺める\n","CBOWにおいて、コンテキスト$w_{t-1}$と$w_{t+1}$が与えられたとき、ターゲットが$w_t$となる確率は、  \n","\n","$$\n","P(w_t|w_{t-1},w_{t+1})\n","$$  \n","\n","左側２つの単語をコンテキストとして考えた場合は、  \n","\n","$$\n","P(w_t|w_{t-2},w_{t-1})\n","$$  \n","\n","その場合の損失関数は、交差エントロピー誤差を適用して、  \n","\n","$$\n","L=-\\log{P(w_t|w_{t-2},w_{t-1})}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"XayOAQkF6Kz0"},"source":["###言語モデル\n","* 言語モデルは、単語の並びに対して確率を与える\n","* 単語の並びに対して、それがどれだけ自然な単語の並びなのか、を確率で評価する\n","\n","$w_1,...,w_m$という順序で単語が出現する確率は同時確率$P(w_1,...,w_m)$で表せるが、この同時確率は事後確率を使って以下のように表せる  \n","\n","$$\n","\\begin{eqnarray}\n","P(w_1,...,w_m) &=& P(w_m|w_1,...,w_{m-1})P(w_{m-1}|w_1,...,w_{m-2})...P(w_3|w_1,w_2)P(w_2|w_1)P(w_1)\\\\&=& \\prod_{t=1}^m P(w_t|w_1,...,w_{t-1})\n","\\end{eqnarray}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"X70Be8ndBr-Z"},"source":["###CBOWモデルを言語モデルに？\n","CBOWモデルを無理やり言語モデルに適用するには、コンテキストのサイズをある値に限定する必要がある  \n","コンテキストのサイズを左側の２つに限定した場合、以下のように近似できる  \n","\n","$$\n","P(w_1,...,w_m)=\\prod_{t=1}^m P(w_t|w_1,...,w_{t-1})\\approx\\prod_{t=1}^m P(w_t|w_{t-2},w_{t-1})\n","$$  \n","* マルコフ性（または「マルコフモデル」、「マルコフ連鎖」）  \n","    未来の状態が現在の状態だけに依存して決まること  \n","* N階マルコフ連鎖  \n","    ある事象の確率がその直前のN個の事象だけに依存するとき、これを「N階マルコフ連鎖」という  \n","    （直前の２単語だけに依存して次の単語が決まるモデルは「２階マルコフ連鎖」と呼ぶことができる）  \n","\n","CBOWモデルではコンテキストのサイズを限定する必要があるため、サイズ外の単語の情報は無視される  \n","CBOWモデルの中間層では単語ベクトルの和が求められるため、コンテキストの単語の並び方は無視される  \n","和ではなく連結することで単語の順番を保持できるが、コンテキストのサイズに比例して重みパラメータが増えるため、それは避けたい  \n","→リカレントニューラルネットワーク（RNN）で解決"]},{"cell_type":"markdown","metadata":{"id":"np2_ifwZHq0F"},"source":["##RNNとは"]},{"cell_type":"markdown","metadata":{"id":"foeXYZP2IBcH"},"source":["###循環するニューラルネットワーク\n","* RNNレイヤはループする（閉じた）経路を持つ\n","* 時刻$t$に入力${\\rm x_t}$を入力したときの出力${\\rm h_t}$はコピーされ、次の時刻$t+1$のレイヤへと入力される"]},{"cell_type":"markdown","metadata":{"id":"8RGzgexZJQ4w"},"source":["###ループの展開\n","各時刻のRNNレイヤはそのレイヤへの入力と一つ前のRNNレイヤからの出力を受け取り、その２つの情報を元にその時刻の出力が計算される(RNNレイヤには$\\rm x$に対する重み$\\rm W_x$と、$\\rm h$に対する重み$\\rm W_h$の２種類の重みがある)  \n","\n","$$\n","{\\rm h}_t=\\tanh({\\rm h}_{t-1}{\\rm W_h}+{\\rm x}_t{\\rm W_x}+{\\rm b})\n","$$\n","\n","* tanh関数（ハイパボリックタンジェント）  \n","tanh関数の出力は、-1から１までの値をとる（シグモイド関数は0から1まで）  \n","\n","$$\n","\\begin{eqnarray}\n","{\\rm f}(x)&=&\\tanh(x)=\\dfrac{{\\rm e}^x-{\\rm e}^{-x}}{{\\rm e}^x+{\\rm e}^{-x}}\\\\\n","{\\rm f'}(x)&=&1-{\\rm f}(x)^2\n","\\end{eqnarray}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"hW426enJVzHP"},"source":["###Backpropagation Trough Time（BPTT）\n","時間方向に展開した誤差逆伝搬法"]},{"cell_type":"markdown","metadata":{"id":"CvsUC0X_WOfo"},"source":["###Truncated BPTT\n","* ネットワークのつながりを適当な長さで断ち切り、小さなネットワークを複数作る\n","* 切り取ったネットワークそれぞれに対して、誤差逆伝播法を行う\n","* 断ち切るのは逆伝搬のつながりだけ(順伝搬の流れは途切れることなく伝搬する)\n","* 1000個の時系列データを扱うとき、RNNレイヤを展開すると横方向に1000個のレイヤが並んだネットワークになる\n","* ネットワークがあまりにも長いと計算量やメモリの使用量が問題になる.さらに勾配消失が発生する\n","* Truncated BPTTによってRNNを学習させるときは、データをシーケンシャル（順番）に与える必要がある\n","    (1000個の時系列データを2つのバッチに分け、Truncated BPTTでネットワークを10個ずつのブロックに断ち切った場合、\n","     最初のブロックの１つ目のバッチには0から9までのデータ、２つ目のバッチには500から509までのデータを与える.\n","     ２つ目のブロックの1つ目のバッチには10から19までのデータ、２つ目のバッチには510から519までのデータを与える)"]},{"cell_type":"markdown","metadata":{"id":"iMKVCYxoZhwt"},"source":["##RNNの実装"]},{"cell_type":"markdown","metadata":{"id":"lkCLGYOKa2b8"},"source":["###RNNレイヤの実装\n","行列の形状チェック(バイアスは省略)\n","$$\n","{\\rm {h}_{t-1}}{\\rm {W_h}}+{\\rm {x}_{t}{\\rm {W_x}}}={\\rm {h_t}}\\\\\n","(N \\times H)(H \\times H)+(N \\times D)(D \\times H)=(N \\times H)\\\\\n","\\begin　{eqnarray}\n","N&:&バッチサイズ\\\\\n","H&:&隠れ状態ベクトルの要素数\\\\\n","D&:&入力ベクトルの次元数（単語の分散表現の次元数）\n","\\end{eqnarray}\n","$$"]},{"cell_type":"code","metadata":{"id":"bVOX79IOjllE","executionInfo":{"status":"ok","timestamp":1608277462223,"user_tz":-540,"elapsed":908,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}}},"source":["class RNN:\n","    def __init__(self,Wx,Wh,b):\n","        self.params=[Wx,Wh,b]\n","        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n","        self.cache=None\n","\n","    def forward(self,x,h_prev):\n","        Wx,Wh,b=self.params\n","        t=np.dot(h_prev,Wh)+np.dot(x,Wx)+b\n","        h_next=np.tanh(t)\n","        self.cache=(x,h_prev,h_next)\n","\n","        return h_next\n","\n","    def backward(self,dh_next):\n","        Wx,Wh,b=self.params\n","        x,h_prev,h_next=self.cache\n","        dt=dh_next*(1.-h_next**2)\n","        db=np.sum(dt,axis=0)    #repeatノードは集約（N×H→H）\n","        dWh=np.dot(h_prev.T,dt) #逆伝搬してきた方(dt)はいじらないで行列の形状が正しくなるように掛ける方を転置する\n","        dh_prev=np.dot(dt,Wh.T)\n","        dWx=np.dot(x.T,dt)\n","        dx=np.dot(dt,Wx.T)\n","\n","        self.grads[0][...]=dWx\n","        self.grads[1][...]=dWh\n","        self.grads[2][...]=db\n","\n","        return dx,dh_prev"],"execution_count":108,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9x7uxn3QQ9I","executionInfo":{"status":"ok","timestamp":1608276169843,"user_tz":-540,"elapsed":2750,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"efc75111-2874-49fb-e942-37bc5c7602d9"},"source":["import numpy as np\n","\n","x=np.random.randn(2,5)\n","print('x',x)\n","print('\\nx.shape\\n',x.shape)\n","N,D=x.shape\n","H=3\n","h_prev=np.random.randn(N,H)\n","\n","Wx=0.01*np.random.randn(D,H)\n","Wh=0.01*np.random.randn(H,H)\n","b=np.zeros(H)\n","\n","print('\\nh_prev\\n',h_prev)\n","print('\\nWx\\n',Wx)\n","print('\\nWh\\n',Wh)\n","print('\\nb\\n',b)\n","\n","network=RNN(Wx,Wh,b)\n","\n","#forward\n","h_next=network.forward(x,h_prev)\n","print('\\nh_next\\n',h_next)\n","\n","#backward\n","dh_next=np.ones((N,H))\n","print('\\ndh_next\\n',dh_next)\n","\n","dx,dh_prev=network.backward(dh_next)\n","dWx=network.grads[0]\n","dWh=network.grads[1]\n","db=network.grads[2]\n","\n","print('\\ndx\\n',dx)\n","print('\\ndh_prev\\n',dh_prev)\n","print('\\ndWx\\n',dWx)\n","print('\\ndWh\\n',dWh)\n","print('\\ndb\\n',db)\n"],"execution_count":84,"outputs":[{"output_type":"stream","text":["x [[ 0.03989551 -1.33753659  2.77781523 -0.36335789 -0.39782286]\n"," [-0.34410591  0.64948128 -1.1256755   0.08680319  0.74699912]]\n","\n","x.shape\n"," (2, 5)\n","\n","h_prev\n"," [[ 1.14163931  0.99142255  1.00784055]\n"," [-0.07737676  0.89871819 -0.82135852]]\n","\n","Wx\n"," [[ 0.00304896 -0.0082288   0.00773326]\n"," [ 0.00775807 -0.02690801  0.00638309]\n"," [ 0.01074696  0.0029286  -0.00733836]\n"," [ 0.01072262  0.00848788  0.01613718]\n"," [ 0.00710483 -0.00360007  0.0026387 ]]\n","\n","Wh\n"," [[-0.0033952  -0.00416211  0.00647666]\n"," [ 0.00465863 -0.0018586   0.01456905]\n"," [-0.00063948 -0.00380815  0.00106478]]\n","\n","b\n"," [0. 0. 0.]\n","\n","h_next\n"," [[0.01297348 0.03171302 0.        ]\n"," [0.00310477 0.         0.02483487]]\n","\n","dh_next\n"," [[1. 1. 1.]\n"," [1. 1. 1.]]\n","\n","dx\n"," [[ 0.00255342 -0.01276685  0.0063372   0.03534768  0.00614345]\n"," [ 0.00255342 -0.01276685  0.0063372   0.03534768  0.00614345]]\n","\n","dh_prev\n"," [[-0.00108065  0.01736908 -0.00338285]\n"," [-0.00108065  0.01736908 -0.00338285]]\n","\n","dWx\n"," [[-0.30421039 -0.30421039 -0.30421039]\n"," [-0.68805531 -0.68805531 -0.68805531]\n"," [ 1.65213973  1.65213973  1.65213973]\n"," [-0.27655471 -0.27655471 -0.27655471]\n"," [ 0.34917626  0.34917626  0.34917626]]\n","\n","dWh\n"," [[1.06426256 1.06426256 1.06426256]\n"," [1.89014074 1.89014074 1.89014074]\n"," [0.18648203 0.18648203 0.18648203]]\n","\n","db\n"," [2. 2. 2.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lMWXzKf9uVR4"},"source":["###Time RNNレイヤの実装\n","* Time RNNレイヤはT個のRNNレイヤから成る（Tは任意の数）\n","* Time RNNレイヤはRNNレイヤの隠れ状態hをメンバ変数として保持し、次のブロックへと値を引き渡す\n","* 隠れ状態hの引き継ぎを行うかどうかは引数statefulで管理する\n","* statefulがFalseのときは隠れ状態hをゼロ行列で初期化する（状態を保持しないモードであり「ステートレス」と呼ばれる）"]},{"cell_type":"code","metadata":{"id":"xSz5UsMMvngC","executionInfo":{"status":"ok","timestamp":1608276646811,"user_tz":-540,"elapsed":2331,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}}},"source":["class TimeRNN:\n","    def __init__(self,Wx,Wh,b,stateful=False):\n","        self.params=[Wx,Wh,b]\n","        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n","        self.layers=None    #複数のRNNレイヤを格納するメンバ変数を初期化\n","        self.h,self.dh=None,None    #hとdhを初期化\n","        self.stateful=stateful  #hを保持するかどうかを管理する引数をメンバ変数に保持\n","\n","    def set_state(self,h):\n","        self.h=h\n","\n","    def reset_state(self):\n","        self.h=None\n","\n","    def forward(self,xs):\n","        Wx,Wh,b=self.params\n","        N,T,D=xs.shape  #Tは１ブロックに含まれるRNNレイヤの数（=時系列データの数）\n","        D,H=Wx.shape\n","\n","        self.layers=[]  #複数のRNNレイヤを格納するリストを作成\n","        hs=np.empty((N,T,H),dtype='f')\n","\n","        if not self.stateful or self.h is None: #statefulがFalseまたはself.hの初回呼び出し時は、self.hをゼロ行列で初期化する\n","            self.h=np.zeros((N,H),dtype='f')\n","\n","        for t in range(T):\n","            layer=RNN(*self.params)\n","            self.h=layer.forward(xs[:,t,:],self.h)  #xsにおける各バッチのt番目のベクトルをforwardの引数として渡す\n","            hs[:,t,:]=self.h\n","            self.layers.append(layer)\n","\n","        return hs\n","\n","    def backward(self,dhs):\n","        Wx,Wh,b=self.params\n","        N,T,H=dhs.shape\n","        D,H=Wx.shape\n","        dxs=np.empty((N,T,D),dtype='f')\n","        dh=0\n","        grads=[0,0,0]   #[dWx,dWh,db]\n","\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            dx,dh=layer.backward(dhs[:,t,:]+dh) #順伝搬の際に分岐（コピー）したhは逆伝搬では合算されて伝搬する\n","            dxs[:,t,:]=dx\n","\n","            for i,grad in enumerate(layer.grads):  #各RNN layerの勾配（dWx,dWh,db）同士を足し合わせる\n","                grads[i]+=grad\n","\n","        for i,grad in enumerate(grads):\n","            self.grads[i][...]=grad\n","        self.dh=dh\n","\n","        return dxs"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ01_T9yvQ2e","executionInfo":{"status":"ok","timestamp":1608009416154,"user_tz":-540,"elapsed":746,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"4b41faee-edb9-4ed0-f904-6b1ba1d36fbb"},"source":["import numpy as np\n","\n","xs=np.random.randn(2,5,5)\n","print('xs\\n',xs)\n","print('\\nxs.shape\\n',xs.shape)\n","N,T,D=xs.shape\n","H=3\n","\n","Wx=0.01*np.random.randn(D,H)\n","Wh=0.01*np.random.randn(H,H)\n","b=np.zeros(H)\n","\n","print('\\nWx\\n',Wx)\n","print('\\nWh\\n',Wh)\n","print('\\nb\\n',b)\n","\n","network=TimeRNN(Wx,Wh,b)\n","\n","#forward\n","hs=network.forward(xs)\n","print('\\nhs\\n',hs)\n","print('\\nlayers\\n',network.layers)\n","\n","# #backward\n","dhs=np.ones((N,T,H))\n","print('\\ndhs\\n',dhs)\n","dxs=network.backward(dhs)\n","print('\\ndxs\\n',dxs)\n","dWx,dWh,db=network.grads\n","print('\\ndWx\\n',dWx)\n","print('\\ndWh\\n',dWh)\n","print('\\ndb\\n',db)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["xs\n"," [[[-0.63618211  0.10934455 -1.30850419 -0.8162692   0.59417823]\n","  [-0.53657741  1.14036832  1.10450334  2.86300854 -0.57297783]\n","  [ 2.06134055 -0.9410649   1.75069735  1.59021476 -0.18281125]\n","  [-0.70080835  0.64823805  0.23866598 -1.55460456 -1.79561519]\n","  [-0.26292543 -0.1222049  -0.44097978 -1.11948802  0.66123873]]\n","\n"," [[-1.07518563 -1.01852707  0.69082961 -1.55398959 -1.54405437]\n","  [-0.75994002  1.31608444 -0.73171119 -1.42754758  1.43147073]\n","  [ 0.48028667 -0.09554531 -0.06117461 -0.0455994   0.75968657]\n","  [ 1.55907695  0.71341106  0.11403865 -2.17180654  0.55244124]\n","  [ 0.41954237 -0.60029538 -0.77939816  0.28686443  0.74967301]]]\n","\n","xs.shape\n"," (2, 5, 5)\n","\n","Wx\n"," [[-0.00960174  0.01181981  0.00405078]\n"," [ 0.00868766 -0.00685055  0.01390331]\n"," [-0.01249879  0.00579172 -0.00018765]\n"," [ 0.00922318 -0.01907987  0.01836455]\n"," [ 0.00112909 -0.00637679  0.002924  ]]\n","\n","Wh\n"," [[ 0.01129489 -0.00034219  0.00012349]\n"," [ 0.00838234  0.00444101 -0.01072788]\n"," [-0.00028483  0.00632926  0.01080189]]\n","\n","b\n"," [0. 0. 0.]\n","\n","hs\n"," [[[ 0.0165539  -0.00406173 -0.01406336]\n","  [ 0.0271636  -0.05877439  0.06418193]\n","  [-0.0355784   0.01191102  0.02492866]\n","  [-0.00729717  0.02998387 -0.02752689]\n","  [-0.0024273   0.01227933 -0.02192319]]\n","\n"," [[-0.02323141  0.03774816 -0.051653  ]\n","  [ 0.01639294 -0.00427819 -0.0076394 ]\n","  [-0.00408837  0.00192983  0.00197793]\n","  [-0.02962642  0.05209151 -0.02205233]\n","  [ 0.00409856 -0.00559468  0.00015912]]]\n","\n","layers\n"," [<__main__.RNN object at 0x7f99a082ba20>, <__main__.RNN object at 0x7f99a07c2e10>, <__main__.RNN object at 0x7f99a07c2eb8>, <__main__.RNN object at 0x7f99a07c2f60>, <__main__.RNN object at 0x7f99a07c2d68>]\n","\n","dhs\n"," [[[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]\n","  [1. 1. 1.]]]\n","\n","dxs\n"," [[[ 0.00625585  0.01605462 -0.00702263  0.00887799 -0.00227524]\n","  [ 0.00620346  0.01601965 -0.00703606  0.00886771 -0.00226512]\n","  [ 0.00626219  0.01604166 -0.00701093  0.00886457 -0.00227656]\n","  [ 0.00624229  0.01605139 -0.00702856  0.00888176 -0.00227192]\n","  [ 0.00626518  0.01573472 -0.00689543  0.00850185 -0.00232416]]\n","\n"," [[ 0.0062315   0.01602815 -0.00702719  0.00885804 -0.00227353]\n","  [ 0.00625628  0.0160578  -0.00702298  0.00888241 -0.00227445]\n","  [ 0.00625408  0.01606035 -0.00702603  0.00888522 -0.00227409]\n","  [ 0.00622989  0.01606056 -0.00702875  0.00891403 -0.00226042]\n","  [ 0.00626865  0.01574049 -0.00689469  0.0085083  -0.00232353]]]\n","\n","dWx\n"," [[ 0.55028894  0.54901734  0.55907857]\n"," [ 1.17046622  1.14881809  1.17913414]\n"," [ 0.59407142  0.57513504  0.60024753]\n"," [-3.98438682 -3.95623428 -4.00730498]\n"," [ 0.64542757  0.6557708   0.64781111]]\n","\n","dWh\n"," [[-0.03977295 -0.03973602 -0.03980413]\n"," [ 0.06643923  0.06651932  0.06631121]\n"," [-0.03171112 -0.03179044 -0.03153587]]\n","\n","db\n"," [10.0852371  10.00742565 10.12626582]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TT8nOMpcUTPp"},"source":["##時系列データを扱うレイヤの実装"]},{"cell_type":"markdown","metadata":{"id":"1T8qJC281mV8"},"source":["###RNNLM(RNN Language Model)の全体図\n","* Embedding→RNN→Affine→Softmax\n","* Embeddingレイヤへ単語ID（ラベル表記）が入力され、該当の単語の分散表現（Embeddingレイヤの重みW_in）を得る  \n","    (単語ID（one-hot表記）と重みW_inとの積は指定した単語ID（ラベル表記）の重みを抜き出すことと等価)\n","* Embeddingレイヤの出力である単語の分散表現がRNNレイヤへの入力となる"]},{"cell_type":"markdown","metadata":{"id":"ZO1izvON4qOC"},"source":["###Timeレイヤの実装\n","* 時系列データ（ここでは各単語を示す）をまとめて処理するレイヤをTime〇〇レイヤとして実装する\n","* 時系列データの数の各レイヤをまとめたものがTime〇〇レイヤ  \n","    (Time〇〇レイヤには時系列データの数の〇〇レイヤが生成される)"]},{"cell_type":"markdown","metadata":{"id":"n5ubPzJk3QNy"},"source":["####Time Embeddingレイヤ"]},{"cell_type":"code","metadata":{"id":"DYUzrT94a3N8"},"source":["class TimeEmbedding:\n","    def __init__(self,W):\n","        self.params=[W]\n","        self.grads=[np.zeros_like(W)]\n","        self.layers=None\n","        self.W=W\n","\n","    def forward(self,xs):\n","        N,T=xs.shape    #N:バッチサイズ,T:時系列データの数\n","        V,D=self.W.shape    #V:vocab_size,D:単語の分散表現の次元数\n","        self.layers=[]\n","        out=np.empty((N,T,D),dtype='f')\n","        for t in range(T):\n","            layer=Embedding(self.W)\n","            out[:,t,:]=layer.forward(xs[:,t])\n","            self.layers.append(layer)\n","\n","        return out\n","\n","    def backward(self,dout):\n","        N,T,D=dout.shape\n","        grad=0\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            layer.backward(dout[:,t,:])\n","            grad+=layer.grads[0]    #すべてのRNNレイヤの勾配を加算する\n","        self.grads[0][...]=grad\n","\n","        return None   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8sNQV4Yrhh-","executionInfo":{"status":"ok","timestamp":1608190336213,"user_tz":-540,"elapsed":668,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"834d94cd-2c74-4f19-8560-489d7ad53d32"},"source":["from common.config import GPU\n","from common.np import *\n","from common.layers import Embedding\n","from common.util import preprocess\n","\n","text='You say goodbye and I say Hello.'\n","corpus,word_to_id,id_to_word=preprocess(text)\n","print('corpus\\n',corpus)\n","V=len(word_to_id)   #語彙数（vocab_size）\n","N=4 #バッチ数\n","T=2 #時系列データ数\n","xs=np.random.randint(0,V,(N,T))\n","print('\\nxs\\n',xs)\n","D=3   #単語の分散表現の次元数（=hidden_size）\n","W=0.01*np.random.randn(V,D)\n","print('\\nW\\n',W)\n","layer=TimeEmbedding(W)\n","\n","#forward\n","out=layer.forward(xs)\n","print('\\nout\\n',out)\n","print('\\nlayers\\n',layer.layers)\n","\n","#backward\n","dout=np.ones_like(out)\n","print('\\ndout\\n',dout)\n","layer.backward(dout)\n","print('\\ndW\\n',layer.grads[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["corpus\n"," [0 1 2 3 4 1 5 6]\n","\n","xs\n"," [[0 2]\n"," [0 4]\n"," [3 5]\n"," [4 6]]\n","\n","W\n"," [[ 0.0053589  -0.00406881 -0.00848264]\n"," [ 0.0009035   0.00286225  0.00539832]\n"," [ 0.00178114  0.00359424  0.00223916]\n"," [-0.00498008 -0.00423294  0.01088062]\n"," [ 0.00783187 -0.0032963   0.00052709]\n"," [ 0.01204696  0.00436061  0.00623441]\n"," [-0.00659795 -0.00813407  0.01806852]]\n","\n","out\n"," [[[ 0.0053589  -0.00406881 -0.00848264]\n","  [ 0.00178114  0.00359424  0.00223916]]\n","\n"," [[ 0.0053589  -0.00406881 -0.00848264]\n","  [ 0.00783187 -0.0032963   0.00052709]]\n","\n"," [[-0.00498008 -0.00423294  0.01088062]\n","  [ 0.01204696  0.00436061  0.00623441]]\n","\n"," [[ 0.00783187 -0.0032963   0.00052709]\n","  [-0.00659795 -0.00813407  0.01806852]]]\n","\n","layers\n"," [<common.layers.Embedding object at 0x7fdfb0f3bac8>, <common.layers.Embedding object at 0x7fdfb0f3ba58>]\n","\n","dout\n"," [[[1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]]]\n","\n","dW\n"," [[2. 2. 2.]\n"," [0. 0. 0.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]\n"," [2. 2. 2.]\n"," [1. 1. 1.]\n"," [1. 1. 1.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TNPS3_i8tbAd"},"source":["####Time Affineレイヤ"]},{"cell_type":"code","metadata":{"id":"NiVUkQLadEUa"},"source":["class Simple_TimeAffine:\n","    def __init__(self,W,b):\n","        self.params=[W,b]\n","        self.grads=[np.zeros_like(W),np.zeros_like(b)]\n","        self.W=W\n","        self.b=b\n","        self.layers=None\n","\n","    def forward(self,xs):\n","        N,T,D=xs.shape\n","        D,M=self.W.shape\n","        self.layers=[]\n","        out=np.empty((N,T,M),dtype='f')\n","\n","        for t in range(T):\n","            layer=Affine(self.W,self.b)\n","            out[:,t,:]=layer.forward(xs[:,t,:])\n","            self.layers.append(layer)\n","\n","        return out\n","\n","    def backward(self,dout):\n","        N,T,M=dout.shape\n","        dW,db=0,0\n","        dxs=np.empty((N,T,D),dtype='f')\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            dxs[:,t,:]=layer.backward(dout[:,t,:])\n","            dW+=layer.grads[0]\n","            db+=layer.grads[1]\n","        \n","        self.grads[0][...]=dW\n","        self.grads[1][...]=db\n","\n","        return dxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2fji1w0nm0h"},"source":["#Simple_TimeAffineの高速版\n","class TimeAffine:\n","    def __init__(self,W,b):\n","        self.params=[W,b]\n","        self.grads=[np.zeros_like(W),np.zeros_like(b)]\n","        self.W=W\n","        self.b=b\n","        self.x=None\n","\n","    def forward(self,x):\n","        N,T,D=x.shape\n","        D,M=self.W.shape\n","        rx=x.reshape(N*T,-1)\n","        out=np.dot(rx,self.W)+self.b\n","        self.x=x\n","\n","        return out.reshape(N,T,-1)\n","\n","    def backward(self,dout):\n","        N,T,M=dout.shape\n","        dout=dout.reshape(N*T,-1)\n","        rx=self.x.reshape(N*T,-1)\n","        db=np.sum(dout,axis=0)\n","        dW=np.dot(rx.T,dout)\n","        dx=np.dot(dout,self.W.T)\n","        dx=dx.reshape(N,T,-1)\n","\n","        self.grads[0][...]=dW\n","        self.grads[1][...]=db\n","\n","        return dx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-0-bkovGfVR","executionInfo":{"status":"ok","timestamp":1608121941277,"user_tz":-540,"elapsed":774,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"b8690f8f-207a-42bc-91ca-a1fb9763c9ef"},"source":["from common.config import GPU\n","from common.np import *\n","from common.layers import Affine\n","\n","N,T,D=2,2,5\n","M=3\n","xs=np.random.randn(N,T,D)\n","print('xs\\n',xs)\n","W=0.01*np.random.randn(D,M)\n","b=np.zeros(M)\n","\n","print('\\nW\\n',W)\n","print('\\nb\\n',b)\n","\n","layer1=Simple_TimeAffine(W,b)\n","layer2=TimeAffine(W,b)\n","\n","#forward\n","out1=layer1.forward(xs)\n","out2=layer2.forward(xs)\n","print('\\nout1\\n',out1)\n","print('\\nout2\\n',out2)\n","\n","#backward\n","dout=np.ones_like(out1)\n","print('\\ndout\\n',dout)\n","dxs1=layer1.backward(dout)\n","dxs2=layer2.backward(dout)\n","print('\\ndxs1\\n',dxs1)\n","print('\\ndxs2\\n',dxs2)\n","dW1=layer1.grads[0]\n","dW2=layer2.grads[0]\n","print('\\ndW1\\n',dW1)\n","print('\\ndW2\\n',dW2)\n","db1=layer1.grads[1]\n","db2=layer2.grads[1]\n","print('\\ndb1\\n',db1)\n","print('\\ndb2\\n',db2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["xs\n"," [[[-1.51866811 -0.19499086  1.0405191  -0.08156836  0.82008436]\n","  [-0.68449064 -1.20016951  1.00249497  0.68873807  0.1383732 ]]\n","\n"," [[ 1.73155296  0.58475799 -0.60211299 -0.31836246  1.11074358]\n","  [ 0.94870972 -0.54283355  1.48037978  0.75495965 -0.87620754]]]\n","\n","W\n"," [[-0.01231775  0.00633112 -0.00113149]\n"," [-0.02379629  0.0131346  -0.01511444]\n"," [ 0.00564761 -0.00633733 -0.01516478]\n"," [-0.0079003   0.00128443  0.01505691]\n"," [ 0.00672265 -0.00333327 -0.00664547]]\n","\n","b\n"," [0. 0. 0.]\n","\n","out1\n"," [[[ 0.03538062 -0.02160845 -0.01779173]\n","  [ 0.03814166 -0.02602709  0.01316248]]\n","\n"," [[-0.0286621   0.01834771 -0.01384158]\n","  [-0.00226278 -0.00661483  0.00187172]]]\n","\n","out2\n"," [[[ 0.03538063 -0.02160845 -0.01779173]\n","  [ 0.03814166 -0.02602709  0.01316248]]\n","\n"," [[-0.0286621   0.01834771 -0.01384158]\n","  [-0.00226278 -0.00661483  0.00187172]]]\n","\n","dout\n"," [[[1. 1. 1.]\n","  [1. 1. 1.]]\n","\n"," [[1. 1. 1.]\n","  [1. 1. 1.]]]\n","\n","dxs1\n"," [[[-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]\n","  [-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]]\n","\n"," [[-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]\n","  [-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]]]\n","\n","dxs2\n"," [[[-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]\n","  [-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]]\n","\n"," [[-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]\n","  [-0.00711811 -0.02577613 -0.01585451  0.00844105 -0.00325609]]]\n","\n","dW1\n"," [[ 0.47710393  0.47710393  0.47710393]\n"," [-1.35323594 -1.35323594 -1.35323594]\n"," [ 2.92128086  2.92128086  2.92128086]\n"," [ 1.0437669   1.0437669   1.0437669 ]\n"," [ 1.1929936   1.1929936   1.1929936 ]]\n","\n","dW2\n"," [[ 0.47710393  0.47710393  0.47710393]\n"," [-1.35323594 -1.35323594 -1.35323594]\n"," [ 2.92128086  2.92128086  2.92128086]\n"," [ 1.0437669   1.0437669   1.0437669 ]\n"," [ 1.1929936   1.1929936   1.1929936 ]]\n","\n","db1\n"," [4. 4. 4.]\n","\n","db2\n"," [4. 4. 4.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SxDoPspxHLNI"},"source":["####Time Softmax With Lossレイヤ\n","* 損失は各時系列データから個別に計算され、その合計を時系列データ数Tで割ることで平均を取った値になる\n","* 各時系列に対して損失を求め、それを合計した時系列における損失の合計をさらに各バッチで合計し、(時系列データ数＊バッチ数)で割ることで、各バッチおよび時系列に関して平均を取った値を損失としている"]},{"cell_type":"code","metadata":{"id":"_fXbElikiaaa"},"source":["class Simple_TimeSoftmaxWithLoss:\n","    def __init__(self):\n","        self.params,self.grads=[],[]\n","        self.cache=None\n","\n","    def forward(self,xs,ts):\n","        N,T,V=xs.shape\n","        layers=[]\n","        loss=0\n","        \n","        for t in range(T):\n","            layer=SoftmaxWithLoss()\n","            loss+=layer.forward(xs[:,t,:],ts[:,t])  #損失の合計を計算\n","            layers.append(layer)\n","        loss/=T #時系列データの平均を取る(バッチの平均はSoftmaxWithLossレイヤで行われている)\n","        self.cache=(layers,xs)\n","\n","        return loss\n","\n","    def backward(self,dout=1):\n","        layers,xs=self.cache\n","        N,T,V=xs.shape\n","        dxs=np.empty((N,T,V),dtype='f')\n","        dout*=1/T\n","\n","        for t in range(T):\n","            layer=layers[t]\n","            dxs[:,t,:]=layer.backward(dout)\n","\n","        return dxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2-y97gcMuuX"},"source":["#Simple_TimeSoftmaxWithLossの高速版\n","class TimeSoftmaxWithLoss:\n","    def __init__(self):\n","        self.params,self.grads=[],[]\n","        self.ignor_label=-1\n","        self.cache=None\n","\n","    def forward(self,xs,ts):\n","        N,T,V=xs.shape\n","\n","        if ts.ndim==3:  #one-hotの場合、ラベル表記に変換する\n","            ts=ts.argmax(axis=2)\n","\n","        mask=(ts!=self.ignor_label) #正解ラベルがignor_labelと等しくないときTrue\n","\n","        #バッチ分と時系列分をまとめる\n","        xs=xs.reshape(N*T,-1)\n","        ts=ts.reshape(N*T)\n","        mask=mask.reshape(N*T)\n","\n","        ys=softmax(xs)\n","        ls=np.log(ys[np.arange(N*T),ts])    #ysの正解ラベルに該当するスコアの対数を取る\n","        ls*=mask    #ignor_labelに該当するデータは損失を0にする\n","        loss=-np.sum(ls)    #損失の合計を計算\n","        loss/=mask.sum()  #maskのTrueの数(N*Tの内のignor_label以外のデータ数)で割り、平均を取る\n","        self.cache=(ts,ys,mask,(N,T,V))\n","\n","        return loss\n","\n","    def backward(self,dout):\n","        ts,ys,mask,(N,T,V)=self.cache\n","        dx=ys\n","        dx[np.arange(N*T),ts]-=1  #softmaxWithLossの逆伝搬はy-t,tは正解ラベル(=1)\n","        dx*=dout\n","        dx/=mask.sum()\n","        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n","        dx = dx.reshape((N, T, V))\n","\n","        return dx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0Gc6gFU49kl","executionInfo":{"status":"ok","timestamp":1608181154373,"user_tz":-540,"elapsed":2759,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"9cbff409-e9ef-4cff-9767-c1cbe61451a2"},"source":["from common.config import GPU\n","from common.np import *\n","from common.layers import SoftmaxWithLoss\n","from common.functions import softmax\n","\n","N,T,V=2,3,5\n","xs=np.random.randn(N,T,V)\n","print('xs\\n',xs)\n","ts=np.random.randint(0,V,(N,T))\n","print('\\nts\\n',ts)\n","\n","layer1=Simple_TimeSoftmaxWithLoss()\n","layer2=TimeSoftmaxWithLoss()\n","\n","#forward\n","loss1=layer1.forward(xs,ts)\n","loss2=layer2.forward(xs,ts)\n","print('\\nloss1\\n',loss1)\n","print('\\nloss2\\n',loss2)\n","\n","#backward\n","dout=1\n","print('\\ndout\\n',dout)\n","dxs1=layer1.backward(dout)\n","dxs2=layer2.backward(dout)\n","print('\\ndxs1\\n',dxs1)\n","print('\\ndxs2\\n',dxs2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["xs\n"," [[[-2.08602089 -0.094807    1.37386671  1.27351202 -1.24890955]\n","  [-0.32100937 -0.95114924  1.63696657  0.08440811 -1.019871  ]\n","  [-1.79221156  0.59913923 -0.74133737  0.10283507  1.63726444]]\n","\n"," [[-0.75334104  1.15184103  0.32636526  0.15766756  0.93976184]\n","  [-0.43855666  0.71078725 -0.76266217  0.25050379 -1.54605386]\n","  [-0.86804302  0.38980687 -1.99782963 -0.6591897  -0.24521999]]]\n","\n","ts\n"," [[2 3 4]\n"," [4 3 0]]\n","\n","loss1\n"," 1.3127503308789004\n","\n","loss2\n"," 1.3127507639176783\n","\n","dout\n"," 1\n","\n","dxs1\n"," [[[ 0.00234006  0.01713958 -0.09222141  0.06733697  0.0054048 ]\n","  [ 0.01570169  0.00836142  0.11124603 -0.14311525  0.00780611]\n","  [ 0.00318664  0.03482436  0.00911426  0.02120025 -0.06832552]]\n","\n"," [[ 0.00896655  0.06026089  0.02639587  0.02229829 -0.1179216 ]\n","  [ 0.02314325  0.07304291  0.0167366  -0.12056894  0.00764618]\n","  [-0.14566903  0.07386655  0.00678438  0.02587464  0.03914345]]]\n","\n","dxs2\n"," [[[ 0.00234006  0.01713958 -0.09222141  0.06733697  0.0054048 ]\n","  [ 0.01570169  0.00836142  0.11124603 -0.14311525  0.00780611]\n","  [ 0.00318664  0.03482437  0.00911426  0.02120025 -0.06832552]]\n","\n"," [[ 0.00896655  0.06026089  0.02639587  0.02229829 -0.1179216 ]\n","  [ 0.02314325  0.07304291  0.0167366  -0.12056894  0.00764618]\n","  [-0.14566902  0.07386655  0.00678438  0.02587464  0.03914345]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H_VaFgJ-B_SR"},"source":["##RNNLMの学習と評価"]},{"cell_type":"markdown","metadata":{"id":"UB-e9tercxP5"},"source":["###RNNLMの実装\n","* Time Embedding → Time RNN → Time Affine → Time SoftmaxWithLoss"]},{"cell_type":"code","metadata":{"id":"Bc8z3FNadG7x","executionInfo":{"status":"ok","timestamp":1608277475493,"user_tz":-540,"elapsed":905,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}}},"source":["class SimpleRnnlm:\n","    def __init__(self,vocab_size,wordvec_size,hidden_size):\n","        V,D,H=vocab_size,wordvec_size,hidden_size\n","        rn=np.random.randn\n","\n","        #重みの初期化\n","        embed_W=(rn(V,D)/100).astype('f')\n","        rnn_Wx=(rn(D,H)/np.sqrt(D)).astype('f') #Xavierの初期値\n","        # rnn_Wx=rn(D,H).astype('f')*np.sqrt(2/D) #Heの初期値\n","        rnn_Wh=(rn(H,H)/np.sqrt(H)).astype('f') #Xavierの初期値\n","        # rnn_Wh=rn(H,H).astype('f')*np.sqrt(2/H) #Heの初期値\n","        rnn_b=np.zeros(H).astype('f')\n","        affine_W=(rn(H,V)/np.sqrt(H)).astype('f')   #Xavierの初期値\n","        # affine_W=rn(H,V).astype('f')*np.sqrt(2/H)   #Heの初期値\n","        affine_b=np.zeros(V).astype('f')\n","\n","        #レイヤの生成\n","        self.layers=[TimeEmbedding(embed_W),\n","                     TimeRNN(rnn_Wx,rnn_Wh,rnn_b,stateful=True),\n","                     TimeAffine(affine_W,affine_b)]\n","        self.loss_layer=TimeSoftmaxWithLoss()\n","        self.rnn_layer=self.layers[1]\n","\n","        #パラメータの格納\n","        self.params,self.grads=[],[]\n","        for layer in self.layers:\n","            self.params+=layer.params\n","            self.grads+=layer.grads\n","\n","    def forward(self,xs,ts):\n","        for layer in self.layers:\n","            xs=layer.forward(xs)\n","        loss=self.loss_layer.forward(xs,ts)\n","        return loss\n","\n","    def backward(self,dout=1):\n","        dout=self.loss_layer.backward(dout)\n","        for layer in reversed(self.layers):\n","            dout=layer.backward(dout)\n","        return dout\n","\n","    def reset_state(self):\n","        self.rnn_layer.reset_state()"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJdHgC8YlXyw","executionInfo":{"status":"ok","timestamp":1608191903693,"user_tz":-540,"elapsed":1017,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"4e56da3e-40d6-40c3-ae26-5158879aaf86"},"source":["from common.config import GPU\n","from common.np import *\n","from common.layers import Embedding\n","from common.util import preprocess\n","\n","text='You say goodbye and I say Hello.'\n","corpus,word_to_id,id_to_word=preprocess(text)\n","print('corpus\\n',corpus)\n","V=len(word_to_id)   #語彙数（vocab_size）\n","D=3 #単語の分散表現の次元数(wordvec_size)\n","H=5 #RNNの隠れ状態ベクトルの要素数(hidden_size)\n","N=4 #バッチ数\n","T=2 #時系列データ数\n","xs=np.random.randint(0,V,(N,T))\n","print('\\nxs\\n',xs)\n","ts=np.random.randint(0,V,(N,T))\n","print('\\nts\\n',ts)\n","\n","network=SimpleRnnlm(V,D,H)\n","print('\\nembed_W\\n',network.params[0])\n","print('\\nrnn_Wx\\n',network.params[1])\n","print('\\nrnn_Wh\\n',network.params[2])\n","print('\\nrnn_b\\n',network.params[3])\n","print('\\naffine_W\\n',network.params[4])\n","print('\\naffine_b\\n',network.params[5])\n","\n","#forward\n","loss=network.forward(xs,ts)\n","print('\\nloss\\n',loss)\n","\n","#backward\n","dout=1\n","print('\\ndout\\n',dout)\n","network.backward(dout)\n","print('\\nembed_dW\\n',network.grads[0])\n","print('\\nrnn_dWx\\n',network.grads[1])\n","print('\\nrnn_dWh\\n',network.grads[2])\n","print('\\nrnn_db\\n',network.grads[3])\n","print('\\naffine_dW\\n',network.grads[4])\n","print('\\naffine_db\\n',network.grads[5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["corpus\n"," [0 1 2 3 4 1 5 6]\n","\n","xs\n"," [[1 3]\n"," [3 3]\n"," [2 6]\n"," [5 1]]\n","\n","ts\n"," [[2 2]\n"," [5 1]\n"," [2 0]\n"," [5 0]]\n","\n","embed_W\n"," [[ 0.00310057 -0.00844028  0.02484088]\n"," [-0.00628202  0.00099597  0.00490091]\n"," [ 0.00596226 -0.01374394  0.0025489 ]\n"," [-0.00697657  0.01079278 -0.02313261]\n"," [ 0.0009852   0.01616624  0.01016055]\n"," [ 0.01645883 -0.00040422 -0.01518514]\n"," [ 0.00541164 -0.01458596  0.00597092]]\n","\n","rnn_Wx\n"," [[-0.8274789  -0.47252423  0.82563287  0.50500035  0.6020962 ]\n"," [ 0.2726653   0.21490891 -0.05997231  0.5129342   0.39015675]\n"," [-0.20823732  0.62016517  0.584389   -0.24629113 -0.57740253]]\n","\n","rnn_Wh\n"," [[-0.37151268 -0.4741759   0.13377102  0.78317213  0.15683983]\n"," [ 0.26014015  0.5402256  -0.02610968 -0.07476007 -0.13132998]\n"," [ 0.42741707 -0.5278082   0.3569692   0.48413485  0.01771144]\n"," [-0.22338991 -0.04342097  0.3617978  -0.00443354  0.27900517]\n"," [ 0.1084639  -0.20848233 -0.05072331  1.0705028  -0.13242473]]\n","\n","rnn_b\n"," [0. 0. 0. 0. 0.]\n","\n","affine_W\n"," [[ 0.47538662 -1.2527825  -0.44768235  0.37886006  0.36203918 -1.0443618\n","   0.14355828]\n"," [ 0.5235328   0.23200642  0.57628906 -0.26049352  0.26788533 -0.43165323\n","   0.13780648]\n"," [ 0.02126406  0.02707593  0.53148377  0.68971014 -0.73828495 -0.03510249\n","   0.66409165]\n"," [-0.05991962  0.12441217 -0.7737476  -0.08126912  0.44889176 -0.2899321\n","   0.13092314]\n"," [-0.44810167 -0.10941838  0.07373254  0.0587365   0.8418794  -0.4055132\n","  -0.20685975]]\n","\n","affine_b\n"," [0. 0. 0. 0. 0. 0. 0.]\n","\n","loss\n"," 1.9454565048217773\n","\n","dout\n"," 1\n","\n","embed_dW\n"," [[ 0.          0.          0.        ]\n"," [ 0.1965392   0.06779382 -0.16150087]\n"," [-0.01561328  0.06314611 -0.13490295]\n"," [-0.09335534  0.1138384  -0.02969618]\n"," [ 0.          0.          0.        ]\n"," [-0.07237499  0.09932972 -0.0514871 ]\n"," [ 0.13786617 -0.0148396  -0.03027372]]\n","\n","rnn_dWx\n"," [[ 1.2551587e-03 -8.3109044e-05 -4.6529120e-04  1.0174983e-03\n","  -4.1838898e-04]\n"," [ 2.1897443e-03  2.4417213e-03  9.9687860e-04 -9.4131747e-04\n","   6.5050903e-05]\n"," [-7.6187896e-03 -2.3042578e-03 -1.3024805e-03 -2.1112745e-03\n","  -4.0932058e-04]]\n","\n","rnn_dWh\n"," [[ 3.5934469e-03  5.4800085e-04 -3.4830396e-04  1.0200095e-04\n","  -9.4127492e-04]\n"," [ 8.4987644e-04  7.6584163e-04 -8.2053110e-04  7.8603951e-04\n","  -1.2962392e-03]\n"," [-3.7152683e-03 -2.2559440e-04  6.6067510e-06  2.4314778e-04\n","   4.3757929e-04]\n"," [ 2.9545833e-04 -2.1082671e-04  4.3238676e-04 -5.3094304e-04\n","   5.0954334e-04]\n"," [ 2.8687518e-04 -5.2392547e-04  7.8055909e-04 -8.8414206e-04\n","   1.0243178e-03]]\n","\n","rnn_db\n"," [ 0.41123816 -0.22379892  0.0795991   0.3308059   0.25714412]\n","\n","affine_dW\n"," [[-2.3449187e-05  4.8819475e-04 -8.1673748e-04  1.7844533e-04\n","   1.9786002e-04 -2.0092806e-04  1.7661481e-04]\n"," [ 1.0971158e-04  6.8067084e-04 -5.2115822e-04 -8.7600329e-04\n","  -8.9595147e-04  2.3737529e-03 -8.7102229e-04]\n"," [-2.0985089e-03  2.0789278e-03  1.3068867e-03 -7.8502472e-04\n","  -8.4185321e-04  1.1185700e-03 -7.7899767e-04]\n"," [ 8.6217257e-04 -2.3090276e-03  1.3151176e-03  6.3576800e-04\n","   6.7356921e-04 -1.8092818e-03  6.3168176e-04]\n"," [ 2.2266174e-03 -1.3710766e-03  2.5585291e-04  7.1031559e-04\n","   7.5112603e-04 -3.2762240e-03  7.0338842e-04]]\n","\n","affine_db\n"," [-0.10753354  0.01776463 -0.2332221   0.14294936  0.14455865 -0.10704345\n","  0.14252643]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ObBnpyWGnBIw"},"source":["###言語モデルの評価\n","* 言語モデルの予測性能の良さを評価する指標として、パープレキシティ(perplexity)がよく用いられる\n","* パープレキシティは簡単に言うと確率の逆数  \n","    (You say...という単語の並びがあり、モデルにYouを入力したときにsayの確率として0.8が出力された場合(sayが正解の場合)、パープレキシティは1/0.8=1.25となる)\n","* パープレキシティが小さいほど予測精度が高いといえる\n","* パープレキシティの値は、分岐数(次に出現しうる単語の候補の数)と解釈できる  \n","    (予測精度が高いモデルほど分岐数が絞り込めている)\n","* 入力データが複数の場合のパープレキシティはモデルの損失を用いて下記のように計算される  \n","\n","$$\n","L=-\\dfrac{1}{N} \\sum_n \\sum_k t_{nk} \\log y_{nk}\\\\\n","\\rm {preplexity} = e^L\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lM6UzMa_KYm-"},"source":["###RNNLMの学習コード"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcKYrR95KjEY","executionInfo":{"status":"ok","timestamp":1608277488235,"user_tz":-540,"elapsed":8892,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"e38403b0-d772-4dc9-dcda-297569e98d3b"},"source":["from common.config import GPU\n","from common.np import *\n","from dataset import ptb\n","from common.optimizer import SGD\n","from common.layers import *\n","from common.util import preprocess\n","\n","#ハイパーパラメータの設定\n","batch_size=10   #バッチサイズ N\n","wordvec_size=100    #単語の分散表現の次元数 D\n","hidden_size=100 #隠れ状態ベクトルの要素数 H\n","time_size=5 #Trancated BPTTの展開する時間サイズ T\n","lr=0.1 #学習率\n","max_epoch=100   #最大エポック\n","\n","#学習データの読み込み\n","corpus,word_to_id,id_to_word=ptb.load_data('train')\n","corpus_size=1000\n","corpus=corpus[:corpus_size]\n","# print(corpus)\n","# text=\"Minecraft is a 3D sandbox game that has no specific goals to accomplish, allowing players a large amount of freedom in choosing how to play the game.However, there is an achievement system,known as 'advancements' in the Java Edition of the game.Gameplay is in the first-person perspective by default, but players have the option for third-person perspective. The game world is composed of rough 3D objects—mainly cubes and fluids, and commonly called 'blocks'—representing various materials, such as dirt, stone, ores, tree trunks, water, and lava. The core gameplay revolves around picking up and placing these objects. These blocks are arranged in a 3D grid, while players can move freely around the world. Players can 'mine' blocks and then place them elsewhere, enabling them to build things.[23] Many commentators have described the game's physics system as unrealistic.[24] The game also contains a material known as redstone, which can be used to make primitive mechanical devices, electrical circuits, and logic gates, allowing for the construction of many complex systems.The default player skin, Steve, stands on a cliffside overlooking a village in a forest. In the distance, there is a small mountain range. The sun is setting to the right, making the sky turn pink and blue.An example of Minecraft's procedurally-generated terrain, including a village and the default skin SteveThe game world is virtually infinite and procedurally generated as players explore it, using a map seed that is obtained from the system clock at the time of world creation (or manually specified by the player).[26][27][28] There are limits on vertical movement, but Minecraft allows an infinitely large game world to be generated on the horizontal plane. Due to technical problems when extremely distant locations are reached, however, there is a barrier preventing players from traversing to locations beyond 30,000,000 blocks from the center.[i] The game achieves this by splitting the world data into smaller sections called 'chunks' that are only created or loaded when players are nearby.[26] The world is divided into biomes ranging from deserts to jungles to snowfields;[29][30] the terrain includes plains, mountains, forests, caves, and various lava/water bodies.[28] The in-game time system follows a day and night cycle, and one full cycle lasts 20 real-time minutes.When starting a new world, players must choose one of five game modes, as well as one of four difficulties, ranging from peaceful to hard. Increasing the difficulty of the game causes the player to take more damage from mobs, as well as having other difficulty-specific effects. For example, the peaceful difficulty prevents hostile mobs from spawning, and the hard difficulty allows players to starve to death if their hunger bar is depleted.[31][32] Once selected, the difficulty can be changed, but the game mode is locked and can only be changed with cheats.\"\n","# corpus,word_to_id,id_to_word=preprocess(text)\n","\n","vocab_size=int(max(corpus)+1)   #語彙数はコーパスの単語IDの最大値＋１個\n","\n","xs=corpus[:-1]  #入力はコーパスの最初の単語から最後の単語の一つ前の単語まで\n","ts=corpus[1:]   #正解ラベルはコーパスの２番目の単語から最後の単語まで\n","data_size=len(xs)\n","print('corpus size: %d, vocab size: %d'%(corpus_size,vocab_size))\n","\n","max_iters=data_size//(batch_size*time_size) #データをすべて見るために必要な繰り返し回数\n","time_idx=0\n","total_loss=0    #損失の合計（損失の移動平均計算用）\n","loss_count=0    #損失を加算した回数（損失の移動平均計算用）\n","ppl_list=[] #計算したパープレキシティを格納するリスト\n","\n","#モデルの生成\n","model=SimpleRnnlm(vocab_size,wordvec_size,hidden_size)\n","optimizer=SGD(lr)\n","\n","#ミニバッチの各サンプルの読み込み開始位置を計算\n","jump=(corpus_size-1)//batch_size\n","offsets=[i*jump for i in range(batch_size)]\n","\n","for epoch in range(max_epoch):\n","    for iter in range(max_iters):\n","        #ミニバッチの取得\n","        batch_x=np.empty((batch_size,time_size),dtype='i')\n","        batch_t=np.empty((batch_size,time_size),dtype='i')\n","        for t in range(time_size):\n","            for i,offset in enumerate(offsets):\n","                #読み込む位置がコーパスのサイズを超えた場合に最初に戻るように、コーパスサイズで割った余りをインデックスにする\n","                batch_x[i,t]=xs[(offset+time_idx)%data_size]\n","                batch_t[i,t]=ts[(offset+time_idx)%data_size]\n","            time_idx+=1\n","\n","        # print('\\nbatch_x\\n',batch_x)\n","        # print('\\nbatch_t\\n',batch_t)\n","        #勾配を求め、パラメータを更新\n","        loss=model.forward(batch_x,batch_t)\n","        model.backward()\n","        optimizer.update(model.params,model.grads)\n","        total_loss+=loss\n","        loss_count+=1\n","\n","    #エポックごとにパープレキシティの評価\n","    # print(loss_count)\n","    ppl=np.exp(total_loss/loss_count)\n","    print('| epoch %d | perplexity %.2f'%(epoch+1,ppl))\n","    ppl_list.append(float(ppl))\n","    total_loss,loss_count=0,0"],"execution_count":110,"outputs":[{"output_type":"stream","text":["corpus size: 1000, vocab size: 418\n","| epoch 1 | perplexity 407.04\n","| epoch 2 | perplexity 294.20\n","| epoch 3 | perplexity 231.31\n","| epoch 4 | perplexity 218.22\n","| epoch 5 | perplexity 207.44\n","| epoch 6 | perplexity 204.06\n","| epoch 7 | perplexity 199.36\n","| epoch 8 | perplexity 197.96\n","| epoch 9 | perplexity 192.31\n","| epoch 10 | perplexity 193.93\n","| epoch 11 | perplexity 188.78\n","| epoch 12 | perplexity 192.77\n","| epoch 13 | perplexity 190.75\n","| epoch 14 | perplexity 191.65\n","| epoch 15 | perplexity 190.51\n","| epoch 16 | perplexity 187.14\n","| epoch 17 | perplexity 185.61\n","| epoch 18 | perplexity 182.24\n","| epoch 19 | perplexity 183.11\n","| epoch 20 | perplexity 184.92\n","| epoch 21 | perplexity 182.05\n","| epoch 22 | perplexity 178.41\n","| epoch 23 | perplexity 176.06\n","| epoch 24 | perplexity 176.79\n","| epoch 25 | perplexity 174.76\n","| epoch 26 | perplexity 175.10\n","| epoch 27 | perplexity 170.64\n","| epoch 28 | perplexity 168.55\n","| epoch 29 | perplexity 167.64\n","| epoch 30 | perplexity 160.89\n","| epoch 31 | perplexity 161.67\n","| epoch 32 | perplexity 157.09\n","| epoch 33 | perplexity 156.09\n","| epoch 34 | perplexity 151.19\n","| epoch 35 | perplexity 152.54\n","| epoch 36 | perplexity 145.99\n","| epoch 37 | perplexity 141.28\n","| epoch 38 | perplexity 137.46\n","| epoch 39 | perplexity 131.59\n","| epoch 40 | perplexity 126.47\n","| epoch 41 | perplexity 126.77\n","| epoch 42 | perplexity 120.01\n","| epoch 43 | perplexity 116.01\n","| epoch 44 | perplexity 113.70\n","| epoch 45 | perplexity 111.78\n","| epoch 46 | perplexity 107.09\n","| epoch 47 | perplexity 100.18\n","| epoch 48 | perplexity 94.43\n","| epoch 49 | perplexity 94.52\n","| epoch 50 | perplexity 90.88\n","| epoch 51 | perplexity 84.53\n","| epoch 52 | perplexity 80.70\n","| epoch 53 | perplexity 76.46\n","| epoch 54 | perplexity 75.00\n","| epoch 55 | perplexity 70.31\n","| epoch 56 | perplexity 66.03\n","| epoch 57 | perplexity 61.80\n","| epoch 58 | perplexity 60.65\n","| epoch 59 | perplexity 55.81\n","| epoch 60 | perplexity 51.64\n","| epoch 61 | perplexity 50.10\n","| epoch 62 | perplexity 47.67\n","| epoch 63 | perplexity 43.22\n","| epoch 64 | perplexity 41.91\n","| epoch 65 | perplexity 39.51\n","| epoch 66 | perplexity 37.13\n","| epoch 67 | perplexity 36.63\n","| epoch 68 | perplexity 32.14\n","| epoch 69 | perplexity 31.02\n","| epoch 70 | perplexity 29.70\n","| epoch 71 | perplexity 28.17\n","| epoch 72 | perplexity 26.05\n","| epoch 73 | perplexity 25.24\n","| epoch 74 | perplexity 23.95\n","| epoch 75 | perplexity 23.13\n","| epoch 76 | perplexity 20.67\n","| epoch 77 | perplexity 19.70\n","| epoch 78 | perplexity 18.96\n","| epoch 79 | perplexity 17.74\n","| epoch 80 | perplexity 16.47\n","| epoch 81 | perplexity 15.29\n","| epoch 82 | perplexity 15.45\n","| epoch 83 | perplexity 14.55\n","| epoch 84 | perplexity 14.02\n","| epoch 85 | perplexity 13.04\n","| epoch 86 | perplexity 11.98\n","| epoch 87 | perplexity 11.95\n","| epoch 88 | perplexity 10.72\n","| epoch 89 | perplexity 10.46\n","| epoch 90 | perplexity 9.55\n","| epoch 91 | perplexity 9.79\n","| epoch 92 | perplexity 9.19\n","| epoch 93 | perplexity 8.63\n","| epoch 94 | perplexity 8.13\n","| epoch 95 | perplexity 7.75\n","| epoch 96 | perplexity 7.63\n","| epoch 97 | perplexity 7.36\n","| epoch 98 | perplexity 6.46\n","| epoch 99 | perplexity 6.16\n","| epoch 100 | perplexity 6.01\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L5Sc1orKYLFq","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1608277492362,"user_tz":-540,"elapsed":857,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"3d0de71f-4849-4ef0-9167-ad59dd7b0450"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(np.arange(max_epoch),ppl_list)\n","plt.show()"],"execution_count":111,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaLQjy_kIXqY","executionInfo":{"status":"ok","timestamp":1608277494364,"user_tz":-540,"elapsed":1111,"user":{"displayName":"yuichiro Shimura","photoUrl":"","userId":"07645022638046915315"}},"outputId":"090c0fb4-f1ba-4583-ecdc-5ed4fa3eecfc"},"source":["from common.util import *\n","\n","querys=['deaths','workers','more','researchers']\n","print(vocab_size)\n","\n","# print(word_to_id)\n","keys=list(word_to_id.keys())\n","vals=list(word_to_id.values())\n","\n","word_to_id_2={}\n","id_to_word_2={}\n","for i in range(vocab_size):\n","    word_to_id_2[keys[i]]=vals[i]\n","    id_to_word_2[vals[i]]=keys[i]\n","print(word_to_id_2)\n","print(id_to_word_2)\n","word_vecs=model.params[4].T\n","\n","for query in querys:\n","    most_similar(query,word_to_id_2,id_to_word_2,word_vecs,top=5)"],"execution_count":112,"outputs":[{"output_type":"stream","text":["418\n","{'aer': 0, 'banknote': 1, 'berlitz': 2, 'calloway': 3, 'centrust': 4, 'cluett': 5, 'fromstein': 6, 'gitano': 7, 'guterman': 8, 'hydro-quebec': 9, 'ipo': 10, 'kia': 11, 'memotec': 12, 'mlx': 13, 'nahb': 14, 'punts': 15, 'rake': 16, 'regatta': 17, 'rubens': 18, 'sim': 19, 'snack-food': 20, 'ssangyong': 21, 'swapo': 22, 'wachter': 23, '<eos>': 24, 'pierre': 25, '<unk>': 26, 'N': 27, 'years': 28, 'old': 29, 'will': 30, 'join': 31, 'the': 32, 'board': 33, 'as': 34, 'a': 35, 'nonexecutive': 36, 'director': 37, 'nov.': 38, 'mr.': 39, 'is': 40, 'chairman': 41, 'of': 42, 'n.v.': 43, 'dutch': 44, 'publishing': 45, 'group': 46, 'rudolph': 47, 'and': 48, 'former': 49, 'consolidated': 50, 'gold': 51, 'fields': 52, 'plc': 53, 'was': 54, 'named': 55, 'this': 56, 'british': 57, 'industrial': 58, 'conglomerate': 59, 'form': 60, 'asbestos': 61, 'once': 62, 'used': 63, 'to': 64, 'make': 65, 'kent': 66, 'cigarette': 67, 'filters': 68, 'has': 69, 'caused': 70, 'high': 71, 'percentage': 72, 'cancer': 73, 'deaths': 74, 'among': 75, 'workers': 76, 'exposed': 77, 'it': 78, 'more': 79, 'than': 80, 'ago': 81, 'researchers': 82, 'reported': 83, 'fiber': 84, 'unusually': 85, 'enters': 86, 'with': 87, 'even': 88, 'brief': 89, 'exposures': 90, 'causing': 91, 'symptoms': 92, 'that': 93, 'show': 94, 'up': 95, 'decades': 96, 'later': 97, 'said': 98, 'inc.': 99, 'unit': 100, 'new': 101, 'york-based': 102, 'corp.': 103, 'makes': 104, 'cigarettes': 105, 'stopped': 106, 'using': 107, 'in': 108, 'its': 109, 'although': 110, 'preliminary': 111, 'findings': 112, 'were': 113, 'year': 114, 'latest': 115, 'results': 116, 'appear': 117, 'today': 118, \"'s\": 119, 'england': 120, 'journal': 121, 'medicine': 122, 'forum': 123, 'likely': 124, 'bring': 125, 'attention': 126, 'problem': 127, 'an': 128, 'story': 129, 'we': 130, \"'re\": 131, 'talking': 132, 'about': 133, 'before': 134, 'anyone': 135, 'heard': 136, 'having': 137, 'any': 138, 'questionable': 139, 'properties': 140, 'there': 141, 'no': 142, 'our': 143, 'products': 144, 'now': 145, 'neither': 146, 'nor': 147, 'who': 148, 'studied': 149, 'aware': 150, 'research': 151, 'on': 152, 'smokers': 153, 'have': 154, 'useful': 155, 'information': 156, 'whether': 157, 'users': 158, 'are': 159, 'at': 160, 'risk': 161, 'james': 162, 'a.': 163, 'boston': 164, 'institute': 165, 'dr.': 166, 'led': 167, 'team': 168, 'from': 169, 'national': 170, 'medical': 171, 'schools': 172, 'harvard': 173, 'university': 174, 'spokeswoman': 175, 'very': 176, 'modest': 177, 'amounts': 178, 'making': 179, 'paper': 180, 'for': 181, 'early': 182, '1950s': 183, 'replaced': 184, 'different': 185, 'type': 186, 'billion': 187, 'sold': 188, 'company': 189, 'men': 190, 'worked': 191, 'closely': 192, 'substance': 193, 'died': 194, 'three': 195, 'times': 196, 'expected': 197, 'number': 198, 'four': 199, 'five': 200, 'surviving': 201, 'diseases': 202, 'including': 203, 'recently': 204, 'total': 205, 'malignant': 206, 'lung': 207, 'far': 208, 'higher': 209, 'rate': 210, 'striking': 211, 'finding': 212, 'those': 213, 'us': 214, 'study': 215, 'west': 216, 'mass.': 217, 'factory': 218, 'appears': 219, 'be': 220, 'highest': 221, 'western': 222, 'industrialized': 223, 'countries': 224, 'he': 225, 'plant': 226, 'which': 227, 'owned': 228, 'by': 229, '&': 230, 'co.': 231, 'under': 232, 'contract': 233, 'probably': 234, 'support': 235, 'argue': 236, 'u.s.': 237, 'should': 238, 'regulate': 239, 'class': 240, 'common': 241, 'kind': 242, 'found': 243, 'most': 244, 'other': 245, 'buildings': 246, 'one': 247, 'few': 248, 'nations': 249, 'does': 250, \"n't\": 251, 'standard': 252, 'regulation': 253, 'smooth': 254, 'fibers': 255, 'such': 256, 'classified': 257, 'according': 258, 't.': 259, 'professor': 260, 'vermont': 261, 'college': 262, 'easily': 263, 'rejected': 264, 'body': 265, 'explained': 266, 'july': 267, 'environmental': 268, 'protection': 269, 'agency': 270, 'imposed': 271, 'gradual': 272, 'ban': 273, 'virtually': 274, 'all': 275, 'uses': 276, 'almost': 277, 'remaining': 278, 'outlawed': 279, 'made': 280, 'areas': 281, 'particularly': 282, 'dusty': 283, 'where': 284, 'dumped': 285, 'large': 286, 'imported': 287, 'material': 288, 'into': 289, 'huge': 290, 'poured': 291, 'cotton': 292, 'mixed': 293, 'dry': 294, 'process': 295, 'described': 296, 'clouds': 297, 'blue': 298, 'dust': 299, 'hung': 300, 'over': 301, 'parts': 302, 'though': 303, 'fans': 304, 'area': 305, 'question': 306, 'some': 307, 'managers': 308, 'contracted': 309, 'phillips': 310, 'vice': 311, 'president': 312, 'human': 313, 'resources': 314, 'but': 315, 'you': 316, 'recognize': 317, 'these': 318, 'events': 319, 'took': 320, 'place': 321, 'bearing': 322, 'work': 323, 'force': 324, 'yields': 325, 'money-market': 326, 'mutual': 327, 'funds': 328, 'continued': 329, 'slide': 330, 'amid': 331, 'signs': 332, 'portfolio': 333, 'expect': 334, 'further': 335, 'declines': 336, 'interest': 337, 'rates': 338, 'average': 339, 'seven-day': 340, 'compound': 341, 'yield': 342, 'taxable': 343, 'tracked': 344, 'money': 345, 'fund': 346, 'report': 347, 'eased': 348, 'fraction': 349, 'point': 350, 'week': 351, 'ended': 352, 'tuesday': 353, 'assume': 354, 'reinvestment': 355, 'dividends': 356, 'current': 357, 'continues': 358, 'maturity': 359, \"'\": 360, 'investments': 361, 'day': 362, 'days': 363, 'longest': 364, 'since': 365, 'august': 366, 'donoghue': 367, 'longer': 368, 'maturities': 369, 'thought': 370, 'indicate': 371, 'declining': 372, 'because': 373, 'they': 374, 'permit': 375, 'retain': 376, 'relatively': 377, 'period': 378, 'shorter': 379, 'considered': 380, 'sign': 381, 'rising': 382, 'can': 383, 'capture': 384, 'sooner': 385, 'open': 386, 'only': 387, 'institutions': 388, 'stronger': 389, 'indicator': 390, 'watch': 391, 'market': 392, 'reached': 393, 'nevertheless': 394, 'editor': 395, 'may': 396, 'again': 397, 'down': 398, 'recent': 399, 'rises': 400, 'short-term': 401, 'six-month': 402, 'treasury': 403, 'bills': 404, 'monday': 405, 'auction': 406, 'example': 407, 'rose': 408, 'despite': 409, 'investors': 410, 'continue': 411, 'pour': 412, 'cash': 413, 'assets': 414, 'grew': 415, '$': 416, 'during': 417}\n","{0: 'aer', 1: 'banknote', 2: 'berlitz', 3: 'calloway', 4: 'centrust', 5: 'cluett', 6: 'fromstein', 7: 'gitano', 8: 'guterman', 9: 'hydro-quebec', 10: 'ipo', 11: 'kia', 12: 'memotec', 13: 'mlx', 14: 'nahb', 15: 'punts', 16: 'rake', 17: 'regatta', 18: 'rubens', 19: 'sim', 20: 'snack-food', 21: 'ssangyong', 22: 'swapo', 23: 'wachter', 24: '<eos>', 25: 'pierre', 26: '<unk>', 27: 'N', 28: 'years', 29: 'old', 30: 'will', 31: 'join', 32: 'the', 33: 'board', 34: 'as', 35: 'a', 36: 'nonexecutive', 37: 'director', 38: 'nov.', 39: 'mr.', 40: 'is', 41: 'chairman', 42: 'of', 43: 'n.v.', 44: 'dutch', 45: 'publishing', 46: 'group', 47: 'rudolph', 48: 'and', 49: 'former', 50: 'consolidated', 51: 'gold', 52: 'fields', 53: 'plc', 54: 'was', 55: 'named', 56: 'this', 57: 'british', 58: 'industrial', 59: 'conglomerate', 60: 'form', 61: 'asbestos', 62: 'once', 63: 'used', 64: 'to', 65: 'make', 66: 'kent', 67: 'cigarette', 68: 'filters', 69: 'has', 70: 'caused', 71: 'high', 72: 'percentage', 73: 'cancer', 74: 'deaths', 75: 'among', 76: 'workers', 77: 'exposed', 78: 'it', 79: 'more', 80: 'than', 81: 'ago', 82: 'researchers', 83: 'reported', 84: 'fiber', 85: 'unusually', 86: 'enters', 87: 'with', 88: 'even', 89: 'brief', 90: 'exposures', 91: 'causing', 92: 'symptoms', 93: 'that', 94: 'show', 95: 'up', 96: 'decades', 97: 'later', 98: 'said', 99: 'inc.', 100: 'unit', 101: 'new', 102: 'york-based', 103: 'corp.', 104: 'makes', 105: 'cigarettes', 106: 'stopped', 107: 'using', 108: 'in', 109: 'its', 110: 'although', 111: 'preliminary', 112: 'findings', 113: 'were', 114: 'year', 115: 'latest', 116: 'results', 117: 'appear', 118: 'today', 119: \"'s\", 120: 'england', 121: 'journal', 122: 'medicine', 123: 'forum', 124: 'likely', 125: 'bring', 126: 'attention', 127: 'problem', 128: 'an', 129: 'story', 130: 'we', 131: \"'re\", 132: 'talking', 133: 'about', 134: 'before', 135: 'anyone', 136: 'heard', 137: 'having', 138: 'any', 139: 'questionable', 140: 'properties', 141: 'there', 142: 'no', 143: 'our', 144: 'products', 145: 'now', 146: 'neither', 147: 'nor', 148: 'who', 149: 'studied', 150: 'aware', 151: 'research', 152: 'on', 153: 'smokers', 154: 'have', 155: 'useful', 156: 'information', 157: 'whether', 158: 'users', 159: 'are', 160: 'at', 161: 'risk', 162: 'james', 163: 'a.', 164: 'boston', 165: 'institute', 166: 'dr.', 167: 'led', 168: 'team', 169: 'from', 170: 'national', 171: 'medical', 172: 'schools', 173: 'harvard', 174: 'university', 175: 'spokeswoman', 176: 'very', 177: 'modest', 178: 'amounts', 179: 'making', 180: 'paper', 181: 'for', 182: 'early', 183: '1950s', 184: 'replaced', 185: 'different', 186: 'type', 187: 'billion', 188: 'sold', 189: 'company', 190: 'men', 191: 'worked', 192: 'closely', 193: 'substance', 194: 'died', 195: 'three', 196: 'times', 197: 'expected', 198: 'number', 199: 'four', 200: 'five', 201: 'surviving', 202: 'diseases', 203: 'including', 204: 'recently', 205: 'total', 206: 'malignant', 207: 'lung', 208: 'far', 209: 'higher', 210: 'rate', 211: 'striking', 212: 'finding', 213: 'those', 214: 'us', 215: 'study', 216: 'west', 217: 'mass.', 218: 'factory', 219: 'appears', 220: 'be', 221: 'highest', 222: 'western', 223: 'industrialized', 224: 'countries', 225: 'he', 226: 'plant', 227: 'which', 228: 'owned', 229: 'by', 230: '&', 231: 'co.', 232: 'under', 233: 'contract', 234: 'probably', 235: 'support', 236: 'argue', 237: 'u.s.', 238: 'should', 239: 'regulate', 240: 'class', 241: 'common', 242: 'kind', 243: 'found', 244: 'most', 245: 'other', 246: 'buildings', 247: 'one', 248: 'few', 249: 'nations', 250: 'does', 251: \"n't\", 252: 'standard', 253: 'regulation', 254: 'smooth', 255: 'fibers', 256: 'such', 257: 'classified', 258: 'according', 259: 't.', 260: 'professor', 261: 'vermont', 262: 'college', 263: 'easily', 264: 'rejected', 265: 'body', 266: 'explained', 267: 'july', 268: 'environmental', 269: 'protection', 270: 'agency', 271: 'imposed', 272: 'gradual', 273: 'ban', 274: 'virtually', 275: 'all', 276: 'uses', 277: 'almost', 278: 'remaining', 279: 'outlawed', 280: 'made', 281: 'areas', 282: 'particularly', 283: 'dusty', 284: 'where', 285: 'dumped', 286: 'large', 287: 'imported', 288: 'material', 289: 'into', 290: 'huge', 291: 'poured', 292: 'cotton', 293: 'mixed', 294: 'dry', 295: 'process', 296: 'described', 297: 'clouds', 298: 'blue', 299: 'dust', 300: 'hung', 301: 'over', 302: 'parts', 303: 'though', 304: 'fans', 305: 'area', 306: 'question', 307: 'some', 308: 'managers', 309: 'contracted', 310: 'phillips', 311: 'vice', 312: 'president', 313: 'human', 314: 'resources', 315: 'but', 316: 'you', 317: 'recognize', 318: 'these', 319: 'events', 320: 'took', 321: 'place', 322: 'bearing', 323: 'work', 324: 'force', 325: 'yields', 326: 'money-market', 327: 'mutual', 328: 'funds', 329: 'continued', 330: 'slide', 331: 'amid', 332: 'signs', 333: 'portfolio', 334: 'expect', 335: 'further', 336: 'declines', 337: 'interest', 338: 'rates', 339: 'average', 340: 'seven-day', 341: 'compound', 342: 'yield', 343: 'taxable', 344: 'tracked', 345: 'money', 346: 'fund', 347: 'report', 348: 'eased', 349: 'fraction', 350: 'point', 351: 'week', 352: 'ended', 353: 'tuesday', 354: 'assume', 355: 'reinvestment', 356: 'dividends', 357: 'current', 358: 'continues', 359: 'maturity', 360: \"'\", 361: 'investments', 362: 'day', 363: 'days', 364: 'longest', 365: 'since', 366: 'august', 367: 'donoghue', 368: 'longer', 369: 'maturities', 370: 'thought', 371: 'indicate', 372: 'declining', 373: 'because', 374: 'they', 375: 'permit', 376: 'retain', 377: 'relatively', 378: 'period', 379: 'shorter', 380: 'considered', 381: 'sign', 382: 'rising', 383: 'can', 384: 'capture', 385: 'sooner', 386: 'open', 387: 'only', 388: 'institutions', 389: 'stronger', 390: 'indicator', 391: 'watch', 392: 'market', 393: 'reached', 394: 'nevertheless', 395: 'editor', 396: 'may', 397: 'again', 398: 'down', 399: 'recent', 400: 'rises', 401: 'short-term', 402: 'six-month', 403: 'treasury', 404: 'bills', 405: 'monday', 406: 'auction', 407: 'example', 408: 'rose', 409: 'despite', 410: 'investors', 411: 'continue', 412: 'pour', 413: 'cash', 414: 'assets', 415: 'grew', 416: '$', 417: 'during'}\n","\n","[query]deaths\n","taxable: 0.4093071520328522\n","yields: 0.32774221897125244\n","august: 0.2750621736049652\n","dusty: 0.2737272381782532\n","dust: 0.2591835558414459\n","\n","[query]workers\n","<unk>: 0.34041884541511536\n","who: 0.33732759952545166\n","for: 0.31793564558029175\n","no: 0.2857253849506378\n","kent: 0.27095600962638855\n","\n","[query]more\n","by: 0.4887203872203827\n","was: 0.39965516328811646\n","with: 0.3646772801876068\n","exposures: 0.3613200783729553\n","<eos>: 0.3251124620437622\n","\n","[query]researchers\n","money: 0.47149327397346497\n","funds: 0.31823235750198364\n","on: 0.30135783553123474\n","brief: 0.30040618777275085\n","years: 0.2861407697200775\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mxGq_Vdv0n-h"},"source":["###RNNLMのTrainerクラス"]},{"cell_type":"code","metadata":{"id":"PZ-SKnqc3XKU"},"source":["class RnnlmTrainer:\n","    def __init__(self, model, optimizer):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.time_idx = None\n","        self.ppl_list = None\n","        self.eval_interval = None\n","        self.current_epoch = 0\n","\n","    def get_batch(self, x, t, batch_size, time_size):\n","        batch_x = np.empty((batch_size, time_size), dtype='i')\n","        batch_t = np.empty((batch_size, time_size), dtype='i')\n","\n","        data_size = len(x)\n","        jump = data_size // batch_size\n","        offsets = [i * jump for i in range(batch_size)]  # バッチの各サンプルの読み込み開始位置\n","\n","        for time in range(time_size):\n","            for i, offset in enumerate(offsets):\n","                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n","                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n","            self.time_idx += 1\n","        return batch_x, batch_t\n","\n","    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n","            max_grad=None, eval_interval=20):\n","        data_size = len(xs)\n","        max_iters = data_size // (batch_size * time_size)\n","        self.time_idx = 0\n","        self.ppl_list = []\n","        self.eval_interval = eval_interval\n","        model, optimizer = self.model, self.optimizer\n","        total_loss = 0\n","        loss_count = 0\n","\n","        start_time = time.time()\n","        for epoch in range(max_epoch):\n","            for iters in range(max_iters):\n","                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n","\n","                # 勾配を求め、パラメータを更新\n","                loss = model.forward(batch_x, batch_t)\n","                model.backward()\n","                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n","                if max_grad is not None:\n","                    clip_grads(grads, max_grad)\n","                optimizer.update(params, grads)\n","                total_loss += loss\n","                loss_count += 1\n","\n","                # パープレキシティの評価\n","                if (eval_interval is not None) and (iters % eval_interval) == 0:\n","                    ppl = np.exp(total_loss / loss_count)\n","                    elapsed_time = time.time() - start_time\n","                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n","                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n","                    self.ppl_list.append(float(ppl))\n","                    total_loss, loss_count = 0, 0\n","\n","            self.current_epoch += 1\n","\n","    def plot(self, ylim=None):\n","        x = numpy.arange(len(self.ppl_list))\n","        if ylim is not None:\n","            plt.ylim(*ylim)\n","        plt.plot(x, self.ppl_list, label='train')\n","        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n","        plt.ylabel('perplexity')\n","        plt.show()\n","\n","\n","def remove_duplicate(params, grads):\n","    '''\n","    パラメータ配列中の重複する重みをひとつに集約し、\n","    その重みに対応する勾配を加算する\n","    '''\n","    params, grads = params[:], grads[:]  # copy list\n","\n","    while True:\n","        find_flg = False\n","        L = len(params)\n","\n","        for i in range(0, L - 1):\n","            for j in range(i + 1, L):\n","                # 重みを共有する場合\n","                if params[i] is params[j]:\n","                    grads[i] += grads[j]  # 勾配の加算\n","                    find_flg = True\n","                    params.pop(j)\n","                    grads.pop(j)\n","                # 転置行列として重みを共有する場合（weight tying）\n","                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n","                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n","                    grads[i] += grads[j].T\n","                    find_flg = True\n","                    params.pop(j)\n","                    grads.pop(j)\n","\n","                if find_flg: break\n","            if find_flg: break\n","\n","        if not find_flg: break\n","\n","    return params, grads"],"execution_count":null,"outputs":[]}]}