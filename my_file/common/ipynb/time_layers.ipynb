{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"time_layers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+F8yzFtBRg43pRwUQX4fz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZFbRWNTx6EZK"},"source":["from common.np import *  # import numpy as np (or import cupy as np)\n","from common.layers import *\n","from common.functions import softmax, sigmoid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVOX79IOjllE"},"source":["class RNN:\n","    def __init__(self,Wx,Wh,b):\n","        self.params=[Wx,Wh,b]\n","        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n","        self.cache=None\n","\n","    def forward(self,x,h_prev):\n","        Wx,Wh,b=self.params\n","        t=np.dot(h_prev,Wh)+np.dot(x,Wx)+b\n","        h_next=np.tanh(t)\n","        self.cache=(x,h_prev,h_next)\n","\n","        return h_next\n","\n","    def backward(self,dh_next):\n","        Wx,Wh,b=self.params\n","        x,h_prev,h_next=self.cache\n","        dt=dh_next*(1.-h_next**2)\n","        db=np.sum(dt,axis=0)    #repeatノードは集約（N×H→H）\n","        dWh=np.dot(h_prev.T,dt) #逆伝搬してきた方(dt)はいじらないで行列の形状が正しくなるように掛ける方を転置する\n","        dh_prev=np.dot(dt,Wh.T)\n","        dWx=np.dot(x.T,dt)\n","        dx=np.dot(dt,Wx.T)\n","\n","        self.grads[0][...]=dWx\n","        self.grads[1][...]=dWh\n","        self.grads[2][...]=db\n","\n","        return dx,dh_prev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xSz5UsMMvngC"},"source":["class TimeRNN:\n","    def __init__(self,Wx,Wh,b,stateful=False):\n","        self.params=[Wx,Wh,b]\n","        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n","        self.layers=None    #複数のRNNレイヤを格納するメンバ変数を初期化\n","        self.h,self.dh=None,None    #hとdhを初期化\n","        self.stateful=stateful  #hを保持するかどうかを管理する引数をメンバ変数に保持\n","\n","    def set_state(self,h):\n","        self.h=h\n","\n","    def reset_state(self):\n","        self.h=None\n","\n","    def forward(self,xs):\n","        Wx,Wh,b=self.params\n","        N,T,D=xs.shape  #Tは１ブロックに含まれるRNNレイヤの数（=時系列データの数）\n","        D,H=Wx.shape\n","\n","        self.layers=[]  #複数のRNNレイヤを格納するリストを作成\n","        hs=np.empty((N,T,H),dtype='f')\n","\n","        if not self.stateful or self.h is None: #statefulがFalseまたはself.hの初回呼び出し時は、self.hをゼロ行列で初期化する\n","            self.h=np.zeros((N,H),dtype='f')\n","\n","        for t in range(T):\n","            layer=RNN(*self.params)\n","            self.h=layer.forward(xs[:,t,:],self.h)  #xsにおける各バッチのt番目のベクトルをforwardの引数として渡す\n","            hs[:,t,:]=self.h\n","            self.layers.append(layer)\n","\n","        return hs\n","\n","    def backward(self,dhs):\n","        Wx,Wh,b=self.params\n","        N,T,H=dhs.shape\n","        D,H=Wx.shape\n","        dxs=np.empty((N,T,D),dtype='f')\n","        dh=0\n","        grads=[0,0,0]   #[dWx,dWh,db]\n","\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            dx,dh=layer.backward(dhs[:,t,:]+dh) #順伝搬の際に分岐（コピー）したhは逆伝搬では合算されて伝搬する\n","            dxs[:,t,:]=dx\n","\n","            for i,grad in enumerate(layer.grads):  #各RNN layerの勾配（dWx,dWh,db）同士を足し合わせる\n","                grads[i]+=grad\n","\n","        for i,grad in enumerate(grads):\n","            self.grads[i][...]=grad\n","        self.dh=dh\n","\n","        return dxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYUzrT94a3N8"},"source":["class TimeEmbedding:\n","    def __init__(self,W):\n","        self.params=[W]\n","        self.grads=[np.zeros_like(W)]\n","        self.layers=None\n","        self.W=W\n","\n","    def forward(self,xs):\n","        N,T=xs.shape    #N:バッチサイズ,T:時系列データの数\n","        V,D=self.W.shape    #V:vocab_size,D:単語の分散表現の次元数\n","        self.layers=[]\n","        out=np.empty((N,T,D),dtype='f')\n","        for t in range(T):\n","            layer=Embedding(self.W)\n","            out[:,t,:]=layer.forward(xs[:,t])\n","            self.layers.append(layer)\n","\n","        return out\n","\n","    def backward(self,dout):\n","        N,T,D=dout.shape\n","        grad=0\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            layer.backward(dout[:,t,:])\n","            grad+=layer.grads[0]    #すべてのRNNレイヤの勾配を加算する\n","        self.grads[0][...]=grad\n","\n","        return None   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiVUkQLadEUa"},"source":["class Simple_TimeAffine:\n","    def __init__(self,W,b):\n","        self.params=[W,b]\n","        self.grads=[np.zeros_like(W),np.zeros_like(b)]\n","        self.W=W\n","        self.b=b\n","        self.layers=None\n","\n","    def forward(self,xs):\n","        N,T,D=xs.shape\n","        D,M=self.W.shape\n","        self.layers=[]\n","        out=np.empty((N,T,M),dtype='f')\n","\n","        for t in range(T):\n","            layer=Affine(self.W,self.b)\n","            out[:,t,:]=layer.forward(xs[:,t,:])\n","            self.layers.append(layer)\n","\n","        return out\n","\n","    def backward(self,dout):\n","        N,T,M=dout.shape\n","        dW,db=0,0\n","        dxs=np.empty((N,T,D),dtype='f')\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            dxs[:,t,:]=layer.backward(dout[:,t,:])\n","            dW+=layer.grads[0]\n","            db+=layer.grads[1]\n","        \n","        self.grads[0][...]=dW\n","        self.grads[1][...]=db\n","\n","        return dxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2fji1w0nm0h"},"source":["#Simple_TimeAffineの高速版\n","class TimeAffine:\n","    def __init__(self,W,b):\n","        self.params=[W,b]\n","        self.grads=[np.zeros_like(W),np.zeros_like(b)]\n","        self.W=W\n","        self.b=b\n","        self.x=None\n","\n","    def forward(self,x):\n","        N,T,D=x.shape\n","        D,M=self.W.shape\n","        rx=x.reshape(N*T,-1)\n","        out=np.dot(rx,self.W)+self.b\n","        self.x=x\n","\n","        return out.reshape(N,T,-1)\n","\n","    def backward(self,dout):\n","        N,T,M=dout.shape\n","        dout=dout.reshape(N*T,-1)\n","        rx=self.x.reshape(N*T,-1)\n","        db=np.sum(dout,axis=0)\n","        dW=np.dot(rx.T,dout)\n","        dx=np.dot(dout,self.W.T)\n","        dx=dx.reshape(N,T,-1)\n","\n","        self.grads[0][...]=dW\n","        self.grads[1][...]=db\n","\n","        return dx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fXbElikiaaa"},"source":["class Simple_TimeSoftmaxWithLoss:\n","    def __init__(self):\n","        self.params,self.grads=[],[]\n","        self.cache=None\n","\n","    def forward(self,xs,ts):\n","        N,T,V=xs.shape\n","        layers=[]\n","        loss=0\n","        \n","        for t in range(T):\n","            layer=SoftmaxWithLoss()\n","            loss+=layer.forward(xs[:,t,:],ts[:,t])  #損失の合計を計算\n","            layers.append(layer)\n","        loss/=T #時系列データの平均を取る(バッチの平均はSoftmaxWithLossレイヤで行われている)\n","        self.cache=(layers,xs)\n","\n","        return loss\n","\n","    def backward(self,dout=1):\n","        layers,xs=self.cache\n","        N,T,V=xs.shape\n","        dxs=np.empty((N,T,V),dtype='f')\n","        dout*=1/T\n","\n","        for t in range(T):\n","            layer=layers[t]\n","            dxs[:,t,:]=layer.backward(dout)\n","\n","        return dxs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2-y97gcMuuX"},"source":["#Simple_TimeSoftmaxWithLossの高速版\n","class TimeSoftmaxWithLoss:\n","    def __init__(self):\n","        self.params,self.grads=[],[]\n","        self.ignor_label=-1\n","        self.cache=None\n","\n","    def forward(self,xs,ts):\n","        N,T,V=xs.shape\n","\n","        if ts.ndim==3:  #one-hotの場合、ラベル表記に変換する\n","            ts=ts.argmax(axis=2)\n","\n","        mask=(ts!=self.ignor_label) #正解ラベルがignor_labelと等しくないときTrue\n","\n","        #バッチ分と時系列分をまとめる\n","        xs=xs.reshape(N*T,-1)\n","        ts=ts.reshape(N*T)\n","        mask=mask.reshape(N*T)\n","\n","        ys=softmax(xs)\n","        ls=np.log(ys[np.arange(N*T),ts])    #ysの正解ラベルに該当するスコアの対数を取る\n","        ls*=mask    #ignor_labelに該当するデータは損失を0にする\n","        loss=-np.sum(ls)    #損失の合計を計算\n","        loss/=mask.sum()  #maskのTrueの数(N*Tの内のignor_label以外のデータ数)で割り、平均を取る\n","        self.cache=(ts,ys,mask,(N,T,V))\n","\n","        return loss\n","\n","    def backward(self,dout):\n","        ts,ys,mask,(N,T,V)=self.cache\n","        dx=ys\n","        dx[np.arange(N*T),ts]-=1  #softmaxWithLossの逆伝搬はy-t,tは正解ラベル(=1)\n","        dx*=dout\n","        dx/=mask.sum()\n","        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n","        dx = dx.reshape((N, T, V))\n","\n","        return dx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzEQ4omT3QXW"},"source":["class LSTM:\n","    def __init__(self,Wx,Wh,b):\n","        self.params=[Wx,Wh,b]   #f,g,i,o４つ分の重みがひとつの重みにまとめられている　Wx.shape=(D,4H),Wh.shape=(H,4H),b.shape=(4H,)\n","        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n","        self.cache=None\n","\n","    def forward(self,x,h_prev,c_prev):\n","        Wx,Wh,b=self.params\n","        H=Wh.shape[0]\n","\n","        A=np.dot(x,Wx)+np.dot(h_prev,Wh)+b  #まとめてAffine変換 A.shapeは(N,4H)\n","        #対応する要素に分割\n","        f=A[:,:H]\n","        g=A[:,H:2*H]\n","        i=A[:,2*H:3*H]\n","        o=A[:,3*H:]\n","\n","        f=sigmoid(f)\n","        g=np.tanh(g)\n","        i=sigmoid(i)\n","        o=sigmoid(o)\n","\n","        c_next=f*c_prev+g*i #アダマール積(要素ごとの積)\n","        h_next=o*np.tanh(c_next)    #アダマール積（要素ごとの積）\n","\n","        self.cache=(x,h_prev,c_prev,i,f,g,o,c_next)\n","\n","        return h_next,c_next\n","\n","    def backward(self,dh_next,dc_next):\n","        Wx,Wh,b=self.params\n","        x,h_prev,c_prev,i,f,g,o,c_next=self.cache\n","\n","        tanh_c_next=np.tanh(c_next)\n","        dtanh_c_next=dh_next*o  #アダマール積の逆伝搬は入力をひっくり返してアダマール積を取る\n","        ds=dc_next+dtanh_c_next*(1-tanh_c_next**2)  #tanhの逆伝搬は1-y**2\n","        dc_prev=ds*f    #アダマール積の逆伝搬は入力をひっくり返してアダマール積を取る\n","\n","        do=dh_next*tanh_c_next\n","        dA_o=do*o*(1-o) #sigmoidの逆伝搬はy(1-y)\n","        di=ds*g\n","        dA_i=di*i*(1-i) #sigmoidの逆伝搬はy(1-y)\n","        dg=ds*i\n","        dA_g=dg*(1-g**2)    ##tanhの逆伝搬は1-y**2\n","        df=ds*c_prev\n","        dA_f=df*f*(1-f) #sigmoidの逆伝搬はy(1-y)\n","\n","        dA=np.hstack((dA_f,dA_g,dA_i,dA_o))   #分割した要素を統合する→dA.shapeは(N,4H)\n","        db=np.sum(dA,axis=0)    #db.shapeは(4H,)\n","        dWh=np.dot(h_prev.T,dA)    #dWh.shapeは(H,4H)\n","        dh_prev=np.dot(dA,Wh.T) #dh_prev.shapeは(N,H)\n","        dWx=np.dot(x.T,dA)  #dWx.shapeは(D,4H)\n","        dx=np.dot(dA,Wx.T)  #dx.shapeは(N,D)\n","\n","        self.grads[0][...]=dWx\n","        self.grads[1][...]=dWh\n","        self.grads[2][...]=db\n","\n","        return dx,dh_prev,dc_prev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-oQZcNeSXl9"},"source":["class TimeLSTM:\n","    def __init__(self,Wx,Wh,b,stateful=False):\n","        self.params=[Wx,Wh,b]\n","        self.grads=[np.zeros_like(Wx),np.zeros_like(Wh),np.zeros_like(b)]\n","        self.layers=None\n","        self.h,self.c=None,None\n","        self.dh=None\n","        self.stateful=stateful\n","\n","    def forward(self,xs):\n","        Wx,Wh,b=self.params\n","        N,T,D=xs.shape\n","        H=Wh.shape[0]\n","        self.layers=[]\n","\n","        if not self.stateful or self.h is None:\n","            self.h=np.zeros((N,H)).astype('f')\n","        if not self.stateful or self.c is None:\n","            self.c=np.zeros((N,H)).astype('f')\n","\n","        hs=np.empty((N,T,H)).astype('f')\n","\n","        for t in range(T):\n","            layer=LSTM(Wx,Wh,b)\n","            self.h,self.c=layer.forward(xs[:,t,:],self.h,self.c)\n","            hs[:,t,:]=self.h\n","            self.layers.append(layer)\n","\n","        return hs\n","\n","    def backward(self,dhs):\n","        Wx,Wh,b=self.params\n","        N,T,H=dhs.shape\n","        D=Wx.shape[0]\n","\n","        dxs=np.empty((N,T,D)).astype('f')\n","        dh,dc=0,0\n","\n","        grads=[0,0,0]\n","        for t in reversed(range(T)):\n","            layer=self.layers[t]\n","            dx,dh,dc=layer.backward(dhs[:,t,:]+dh,dc)\n","            dxs[:,t,:]=dx\n","            for i,grad in enumerate(layer.grads):\n","                grads[i]+=grad\n","\n","        for i,grad in enumerate(grads):\n","            self.grads[i][...]=grad\n","\n","        self.dh=dh\n","\n","        return dxs\n","\n","    def set_state(self,h,c=None):\n","        self.h,self.c=h,c\n","\n","    def reset_state(self):\n","        self.h,self.c=None,None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pAwWojpOWs6p"},"source":["class TimeDropout:\n","    def __init__(self, dropout_ratio=0.5):\n","        self.params, self.grads = [], []\n","        self.dropout_ratio = dropout_ratio\n","        self.mask = None\n","        self.train_flg = True\n","\n","    def forward(self, xs):\n","        if self.train_flg:\n","            flg = np.random.rand(*xs.shape) > self.dropout_ratio    #0から1までの乱数を一様分布から生成し、ドロップアウト率と比較した結果をブール値で格納\n","            #ドロップアウト率だけ出力が小さくなるため、スケールをもとに戻す\n","            #（dropout_ratio=1/5の場合、1/5のニューロンが無視されるから出力は4/5になる→5/4を掛けて出力を1に戻す）\n","            scale = 1 / (1.0 - self.dropout_ratio)\n","            self.mask = flg.astype(np.float32) * scale  #flgのうちTrueの箇所はTrue(=1)*scale=scale,Falseの箇所はFalse(=0)*scale=0としたものをmaskとする\n","\n","            return xs * self.mask\n","        else:\n","            return xs\n","\n","    def backward(self, dout):\n","        return dout * self.mask #順伝搬時に無視したニューロンでは逆伝搬もストップする(伝搬する値を0にする)"],"execution_count":null,"outputs":[]}]}